{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CFPB_Complain_text_product_issue_cat_clf_flask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udihu8cFWAL9"
      },
      "source": [
        "# Humanizing Customer Complaints \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qP_fbirVge"
      },
      "source": [
        "### **Problem Defination:** a supervised (labelled) text multi-class classification problem, of which our goal is to make a prediction (assign to correct category) with a new input (complaint)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Hb2TCauiGea"
      },
      "source": [
        "### Kaggle auth token file upload(seperately downloaded from kaggle account)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfwo7ORAmNIG",
        "outputId": "20547dc1-9b8a-4bab-e6aa-a54310cfa513",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c9396fdb-78e2-4c36-a9e5-735499e6ae60\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c9396fdb-78e2-4c36-a9e5-735499e6ae60\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"koushiktcs12\",\"key\":\"d8bf4887882fc2016aa969ef44d835c9\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmHb7wXT4rXa"
      },
      "source": [
        "## Copy uploaded kaggle.json file into google colab woking directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkX0BLZ6mVT-",
        "outputId": "ccb6e176-8663-4e29-85cc-0248b5f23c4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets list"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "ref                                                     title                                              size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------  -----------------------------------------------  ------  -------------------  -------------  \n",
            "unanimad/us-election-2020                               US Election 2020                                  429KB  2020-11-19 10:31:50           6543  \n",
            "antgoldbloom/covid19-data-from-john-hopkins-university  COVID-19 data from John Hopkins University          2MB  2020-11-19 06:04:10           2854  \n",
            "manchunhui/us-election-2020-tweets                      US Election 2020 Tweets                           353MB  2020-11-09 18:51:59           2945  \n",
            "headsortails/us-election-2020-presidential-debates      US Election 2020 - Presidential Debates           199MB  2020-10-23 16:56:10            525  \n",
            "etsc9287/2020-general-election-polls                    Election, COVID, and Demographic Data by County  1020KB  2020-11-14 19:52:25           1065  \n",
            "radustoicescu/2020-united-states-presidential-election  2020 United States presidential election           11MB  2019-07-04 15:00:45            823  \n",
            "shivamb/netflix-shows                                   Netflix Movies and TV Shows                       971KB  2020-01-20 07:33:56          61381  \n",
            "terenceshin/covid19s-impact-on-airport-traffic          COVID-19's Impact on Airport Traffic              106KB  2020-10-19 12:40:17           4682  \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019   Amazon Top 50 Bestselling Books 2009 - 2019        15KB  2020-10-13 09:39:21           4996  \n",
            "nehaprabhavalkar/indian-food-101                        Indian Food 101                                     7KB  2020-09-30 06:23:43           7768  \n",
            "karangadiya/fifa19                                      FIFA 19 complete player dataset                     2MB  2018-12-21 03:52:59         104839  \n",
            "heeraldedhia/groceries-dataset                          Groceries dataset                                 257KB  2020-09-17 04:36:08           8195  \n",
            "andrewmvd/trip-advisor-hotel-reviews                    Trip Advisor Hotel Reviews                          5MB  2020-09-30 08:31:20           5412  \n",
            "docstein/brics-world-bank-indicators                    BRICS World Bank Indicators                         4MB  2020-10-22 12:18:40           1247  \n",
            "omarhanyy/500-greatest-songs-of-all-time                500 Greatest Songs of All Time                    114KB  2020-11-19 19:08:28           1646  \n",
            "google/tinyquickdraw                                    QuickDraw Sketches                                 11GB  2018-04-18 19:38:04           2484  \n",
            "datasnaek/youtube-new                                   Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         116203  \n",
            "uciml/mushroom-classification                           Mushroom Classification                            34KB  2016-12-01 23:08:00          55094  \n",
            "anikannal/solar-power-generation-data                   Solar Power Generation Data                         2MB  2020-08-18 15:52:03          10337  \n",
            "zynicide/wine-reviews                                   Wine Reviews                                       51MB  2017-11-27 17:08:04         119646  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTEx9cLb46mZ"
      },
      "source": [
        "## Download pre-trained word embedding dictionary from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhDv0MV_5IsX",
        "outputId": "ad6229bb-b0cc-4b9e-d7e7-f8e8133952dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle datasets download -d watts2/glove6b50dtxt\n",
        "!kaggle datasets download -d danielwillgeorge/glove6b100dtxt\n",
        "!kaggle datasets download -d incorpes/glove6b200d\n",
        "!kaggle datasets download -d thanakomsn/glove6b300dtxt\n",
        "!kaggle datasets download -d leadbest/googlenewsvectorsnegative300\n",
        "!unzip \\*.zip  && rm *.zip\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading glove6b50dtxt.zip to /content\n",
            " 69% 47.0M/67.7M [00:00<00:00, 35.0MB/s]\n",
            "100% 67.7M/67.7M [00:01<00:00, 69.8MB/s]\n",
            "Downloading glove6b100dtxt.zip to /content\n",
            " 93% 122M/131M [00:01<00:00, 74.1MB/s]\n",
            "100% 131M/131M [00:01<00:00, 69.5MB/s]\n",
            "Downloading glove6b200d.zip to /content\n",
            "100% 258M/259M [00:03<00:00, 50.5MB/s]\n",
            "100% 259M/259M [00:03<00:00, 76.4MB/s]\n",
            "Downloading glove6b300dtxt.zip to /content\n",
            " 98% 377M/386M [00:05<00:00, 47.9MB/s]\n",
            "100% 386M/386M [00:05<00:00, 71.8MB/s]\n",
            "Downloading googlenewsvectorsnegative300.zip to /content\n",
            "100% 3.17G/3.17G [00:45<00:00, 126MB/s]\n",
            "100% 3.17G/3.17G [00:45<00:00, 74.6MB/s]\n",
            "Archive:  googlenewsvectorsnegative300.zip\n",
            "  inflating: GoogleNews-vectors-negative300.bin  \n",
            "  inflating: GoogleNews-vectors-negative300.bin.gz  \n",
            "\n",
            "Archive:  glove6b100dtxt.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "\n",
            "Archive:  glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n",
            "\n",
            "Archive:  glove6b50dtxt.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "\n",
            "Archive:  glove6b200d.zip\n",
            "  inflating: glove.6B.200d.txt       \n",
            "\n",
            "5 archives were successfully processed.\n",
            "glove.6B.100d.txt  glove.6B.50d.txt\t\t\t  kaggle.json\n",
            "glove.6B.200d.txt  GoogleNews-vectors-negative300.bin\t  sample_data\n",
            "glove.6B.300d.txt  GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTF2tvdVl_eP"
      },
      "source": [
        "## Installation of required packages before import in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_10FzJ0RWL7D",
        "outputId": "9b7e482e-1b6c-440b-eeaf-fa44a31cf03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install spacy-langdetect\n",
        "!pip install language-detector\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy-langdetect\n",
            "  Downloading https://files.pythonhosted.org/packages/29/70/72dad19abe81ca8e85ff951da170915211d42d705a001d7e353af349a704/spacy_langdetect-0.1.2-py3-none-any.whl\n",
            "Collecting langdetect==1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (50.3.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (20.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (8.6.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (0.7.1)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993461 sha256=6529c25e430b519cc6728128e11d51bba514b85c9e8bd47de79975598fdf5cc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, spacy-langdetect\n",
            "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\n",
            "Collecting language-detector\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/0a/fefb61145a386968d2070323b608be6a1eb7508e610c2319daad746c2c33/language-detector-5.0.2.tar.gz\n",
            "Building wheels for collected packages: language-detector\n",
            "  Building wheel for language-detector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for language-detector: filename=language_detector-5.0.2-cp36-none-any.whl size=7055 sha256=62ca451af1885e291b56a7c7b59f6b47fa905633650e9960e59ee1fa83c04dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/37/fa/2098a4aa6c0d94d6ddff0d3a79669e12bc4f7baca8a760b3db\n",
            "Successfully built language-detector\n",
            "Installing collected packages: language-detector\n",
            "Successfully installed language-detector-5.0.2\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN8DVPf0WAMC"
      },
      "source": [
        "# Section 1: Importing Packages and Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygeQUvBPWAMH"
      },
      "source": [
        "# Import libraries\n",
        "# This is used for fast string concatination\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 1000\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from tqdm.gui import trange, tqdm\n",
        "from pprint import pprint\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "from nltk.tag import pos_tag\n",
        "import collections as co\n",
        "from collections import OrderedDict\n",
        "import pickle # for saving and loading objects\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "# Classifiers\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Metrics to score classifiers\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, auc, roc_curve, log_loss\n",
        "\n",
        "# Data splitting, CV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
        "\n",
        "# Lifesaver\n",
        "from google.colab import files\n",
        "seed =300"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVtCkz7CrsKc"
      },
      "source": [
        "### **Data Source** Kaggle https://www.kaggle.com/cfpb/us-consumer-finance-complaints\n",
        "### Original data source: https://files.consumerfinance.gov/ccdb/complaints.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOu0wnxTmaNJ"
      },
      "source": [
        "## Load the input complaints text dataset(CFPB data from kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoslckA1qvJ7",
        "outputId": "d35a8cd3-ca0e-477c-a4d4-cef379c92d66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle datasets download -d cfpb/us-consumer-finance-complaints\n",
        "!kaggle datasets download -d koushiktcs12/cfpb-complaints\n",
        "!unzip \\*.zip  && rm *.zip\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading us-consumer-finance-complaints.zip to /content\n",
            " 85% 72.0M/84.5M [00:00<00:00, 58.2MB/s]\n",
            "100% 84.5M/84.5M [00:01<00:00, 87.1MB/s]\n",
            "Downloading cfpb-complaints.zip to /content\n",
            " 94% 267M/285M [00:03<00:00, 51.0MB/s]\n",
            "100% 285M/285M [00:03<00:00, 78.4MB/s]\n",
            "Archive:  cfpb-complaints.zip\n",
            "  inflating: complaints.csv          \n",
            "\n",
            "Archive:  us-consumer-finance-complaints.zip\n",
            "  inflating: consumer_complaints.csv  \n",
            "  inflating: database.sqlite         \n",
            "\n",
            "2 archives were successfully processed.\n",
            "complaints.csv\t\t glove.6B.50d.txt\n",
            "consumer_complaints.csv  GoogleNews-vectors-negative300.bin\n",
            "database.sqlite\t\t GoogleNews-vectors-negative300.bin.gz\n",
            "glove.6B.100d.txt\t kaggle.json\n",
            "glove.6B.200d.txt\t sample_data\n",
            "glove.6B.300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xI-P25sqkdX",
        "outputId": "8b076b67-a829-45aa-c7f4-befe3f81cb9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# # Load CSV file\n",
        "df1= pd.read_csv(r'consumer_complaints.csv')\n",
        "# This step is not executable because the .csv file is not in the drive\n",
        "df2= pd.read_csv(r'complaints.csv')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZfavHBkopC0"
      },
      "source": [
        "# Column Cleaning First. Columns are super nastily named\n",
        "df2.rename(columns={'Date received':'DATE_RECEIVED',\n",
        "                  'Product':'PRODUCT',\n",
        "                  'Sub-product':'SUB_PRODUCT',\n",
        "                  'Issue':'ISSUE',\n",
        "                  'Sub-issue':'SUB_ISSUE',\n",
        "                  'Consumer complaint narrative':'CONSUMER_COMPLAINT_NARRATIVE',\n",
        "                  'Company public response':'COMPANY_PUBLIC_RESPONSE',\n",
        "                   'Company':'COMPANY',\n",
        "                   'State':'STATE',\n",
        "                   'ZIP code':'ZIP_CODE',\n",
        "                   'Tags':'TAGS',\n",
        "                   'Consumer consent provided?':'CONSUMER_CONSENT_PROVIDED',\n",
        "                   'Subbmited via':'SUBMITTED_VIA',\n",
        "                   'Date sent to company':'DATE_SENT_TO_COMPANY',\n",
        "                   'Company response to consumer':'COMPANY_RESPONSE_TO_CONSUMER',\n",
        "                   'Timely response?':'TIMELY_RESPONSE',\n",
        "                   'Consumer disputed?':'CONSUMER_DISPUTED',\n",
        "                   'Complaint ID':'COMPLAINT_ID'\n",
        "                  }, inplace=True)\n",
        "\n",
        "                  # Column Cleaning First. Columns are super nastily named\n",
        "df1.rename(columns={'date_received':'DATE_RECEIVED',\n",
        "                  'product':'PRODUCT',\n",
        "                  'sub_product':'SUB_PRODUCT',\n",
        "                  'issue':'ISSUE',\n",
        "                  'sub_issue':'SUB_ISSUE',\n",
        "                  'consumer_complaint_narrative':'CONSUMER_COMPLAINT_NARRATIVE',\n",
        "                  'company_public_response':'COMPANY_PUBLIC_RESPONSE',\n",
        "                   'company':'COMPANY',\n",
        "                   'state':'STATE',\n",
        "                   'zipcode':'ZIP_CODE',\n",
        "                   'tags':'TAGS',\n",
        "                   'consumer_consent_provided':'CONSUMER_CONSENT_PROVIDED',\n",
        "                   'submitted_via':'SUBMITTED_VIA',\n",
        "                   'date_sent_to_company':'DATE_SENT_TO_COMPANY',\n",
        "                   'company_response_to_consumer':'COMPANY_RESPONSE_TO_CONSUMER',\n",
        "                   'timely_response':'TIMELY_RESPONSE',\n",
        "                   'consumer_disputed?':'CONSUMER_DISPUTED',\n",
        "                   'complaint_id':'COMPLAINT_ID'\n",
        "                  }, inplace=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLBgkkQuopC9",
        "outputId": "9d61b556-837d-496a-a5b8-23d50e90bb34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "frames = [df1, df2]\n",
        "df = pd.concat(frames)\n",
        "print(df.info())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(555957, 18)\n",
            "(1835193, 18)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2391150 entries, 0 to 1835192\n",
            "Data columns (total 19 columns):\n",
            " #   Column                        Dtype \n",
            "---  ------                        ----- \n",
            " 0   DATE_RECEIVED                 object\n",
            " 1   PRODUCT                       object\n",
            " 2   SUB_PRODUCT                   object\n",
            " 3   ISSUE                         object\n",
            " 4   SUB_ISSUE                     object\n",
            " 5   CONSUMER_COMPLAINT_NARRATIVE  object\n",
            " 6   COMPANY_PUBLIC_RESPONSE       object\n",
            " 7   COMPANY                       object\n",
            " 8   STATE                         object\n",
            " 9   ZIP_CODE                      object\n",
            " 10  TAGS                          object\n",
            " 11  CONSUMER_CONSENT_PROVIDED     object\n",
            " 12  SUBMITTED_VIA                 object\n",
            " 13  DATE_SENT_TO_COMPANY          object\n",
            " 14  COMPANY_RESPONSE_TO_CONSUMER  object\n",
            " 15  TIMELY_RESPONSE               object\n",
            " 16  CONSUMER_DISPUTED             object\n",
            " 17  COMPLAINT_ID                  int64 \n",
            " 18  Submitted via                 object\n",
            "dtypes: int64(1), object(18)\n",
            "memory usage: 364.9+ MB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHkPYT-_eaR4"
      },
      "source": [
        "### Section 1.0.1: Dropping all rows that do not have Customer Complaint entries in them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egtEZx5VYgRU"
      },
      "source": [
        "df = df[df[\"CONSUMER_COMPLAINT_NARRATIVE\"].notnull() == True]\n",
        "df = df.drop_duplicates()\n",
        "df['Text_Length'] = df[\"CONSUMER_COMPLAINT_NARRATIVE\"].str.len()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECdQzgUvl5_0"
      },
      "source": [
        "## Subsetting source data columns for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTfvrfDQ_Ri2",
        "outputId": "ce9756ef-6f71-444b-a467-1cda3c18222b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Subsetting dataframe into columns useful for our text multi-classification problem\n",
        "df_product_and_complaint = df[['PRODUCT','ISSUE', 'CONSUMER_COMPLAINT_NARRATIVE','COMPLAINT_ID']]\n",
        "# Renaming columns to something easier\n",
        "df_product_and_complaint.rename(columns={'CONSUMER_COMPLAINT_NARRATIVE':'CONSUMER_COMPLAINT'},inplace=True) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9PfoP2Glvxq"
      },
      "source": [
        "## Dropping multilevel product category data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhqS4IBx_Ri3",
        "outputId": "174ca477-30da-412c-9146-325ee18ef977",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Dropping rows with the aggregated columns\n",
        "df_product_and_complaint.drop(\n",
        "    df_product_and_complaint[\n",
        "    df_product_and_complaint.PRODUCT == \n",
        "    'Credit reporting, credit repair services, or other personal consumer reports'].index, \n",
        "    inplace=True) # credit_aggregated"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCdGNSjsmEbu"
      },
      "source": [
        "## Merging all diffrent type of loans category into single 'loan' category\n",
        "### Rename multinaming level to most appropriate level name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY-dPJi-_Ri2",
        "outputId": "fc532627-8921-4f4b-8882-36da898d0912",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Combining Loans together to make this a simplier classification problem\n",
        "df_product_and_complaint.replace('Student loan', 'Loan', inplace=True)\n",
        "df_product_and_complaint.replace('Consumer Loan', 'Loan', inplace=True)\n",
        "df_product_and_complaint.replace('Payday loan', 'Loan', inplace=True)\n",
        "df_product_and_complaint.replace('Vehicle loan or lease', 'Loan', inplace=True)\n",
        "df_product_and_complaint.replace('Credit card or prepaid card', 'Credit card', inplace=True)\n",
        "df_product_and_complaint.replace('Payday loan, title loan, or personal loan', 'Loan', inplace=True)\n",
        "df_product_and_complaint.replace('Money transfer, virtual currency, or money service', 'Money transfers', inplace=True)\n",
        "# Placing Virtual currency into other financial service\n",
        "df_product_and_complaint.replace('Virtual currency', 'Other financial service', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4389: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  method=method,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6TTye10_Ri2",
        "outputId": "d0cadae9-80b7-44f1-ca60-5cb67b3676d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df_product_and_complaint['PRODUCT'].value_counts())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Debt collection                139993\n",
            "Mortgage                        82530\n",
            "Credit card                     70060\n",
            "Loan                            62830\n",
            "Credit reporting                41404\n",
            "Checking or savings account     26195\n",
            "Bank account or service         19386\n",
            "Money transfers                 13537\n",
            "Prepaid card                     2104\n",
            "Other financial service           390\n",
            "Name: PRODUCT, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRh9aTMtofH0"
      },
      "source": [
        "### We have a wooping 458,429 entries in our dataframe.\n",
        "#### This might cause our comptuer to crash. So we gotta reduce it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi13s-ie_Ri6"
      },
      "source": [
        "# Reducing dataframe to 36k entries\n",
        "# Need to reduce top 8 categories by 80% and for other 2 kept remain same\n",
        "\n",
        "percentage_reduction_0 = 0.97\n",
        "percentage_reduction_1 = 0.95\n",
        "percentage_reduction_2 = 0.0\n",
        "\n",
        "# Debt Collection Reduction\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint.drop(\n",
        "  df_product_and_complaint[df_product_and_complaint['PRODUCT'] == 'Debt collection'].sample(frac=percentage_reduction_0).index)\n",
        "\n",
        "# Mortgage Reduction\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "  df_product_and_complaint_reduced[\n",
        "      df_product_and_complaint_reduced['PRODUCT'] == 'Mortgage'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Loan\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "  df_product_and_complaint_reduced[\n",
        "      df_product_and_complaint_reduced['PRODUCT'] == 'Loan'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Credit reporting\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "  df_product_and_complaint_reduced[\n",
        "      df_product_and_complaint_reduced['PRODUCT'] == 'Credit reporting'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Credit card\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "  df_product_and_complaint_reduced[\n",
        "      df_product_and_complaint_reduced['PRODUCT'] == 'Credit card'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Checking or savings account\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "  df_product_and_complaint_reduced[\n",
        "      df_product_and_complaint_reduced['PRODUCT'] == 'Checking or savings account'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Bank account or service\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "    df_product_and_complaint_reduced[\n",
        "        df_product_and_complaint_reduced['PRODUCT'] == 'Bank account or service'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "#Money transfers \n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "    df_product_and_complaint_reduced[\n",
        "        df_product_and_complaint_reduced['PRODUCT'] == 'Money transfers'].sample(frac=percentage_reduction_1).index)\n",
        "\n",
        "# Prepaid card (too little to be reduced)\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "    df_product_and_complaint_reduced[\n",
        "        df_product_and_complaint_reduced['PRODUCT'] == 'Prepaid card'].sample(frac=percentage_reduction_2).index)\n",
        "\n",
        "# Other financial service (too little to be reduced)\n",
        "df_product_and_complaint_reduced = \\\n",
        "df_product_and_complaint_reduced.drop(\n",
        "    df_product_and_complaint_reduced[\n",
        "        df_product_and_complaint_reduced['PRODUCT'] == 'Other financial service'].sample(frac=percentage_reduction_2).index)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mAR-VDcD5DB",
        "outputId": "abe46dfe-35fe-4451-96d4-a1260db090f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df_product_and_complaint_reduced['PRODUCT'].value_counts())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Debt collection                3966\n",
            "Mortgage                       3899\n",
            "Credit card                    3304\n",
            "Loan                           3012\n",
            "Prepaid card                   1967\n",
            "Credit reporting               1943\n",
            "Checking or savings account    1239\n",
            "Bank account or service         917\n",
            "Money transfers                 633\n",
            "Other financial service         364\n",
            "Name: PRODUCT, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBBEqTYAK1dI"
      },
      "source": [
        "df_product_and_complaint_reduced.index = df_product_and_complaint_reduced.COMPLAINT_ID\n",
        "df = df_product_and_complaint_reduced\n",
        "df = df.rename(columns={\"CONSUMER_COMPLAINT\": \"Complain_text\"})\n",
        "df = df.set_index('COMPLAINT_ID')\n",
        "df['COMPLAINT_ID'] = df.index\n",
        "complain_text = df[['Complain_text']]\n",
        "texts = complain_text.drop_duplicates()\n",
        "df_texts = pd.DataFrame(texts,columns=['Complain_text'])\n",
        "df_texts = df_texts[df_texts['Complain_text'].notnull() == True]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWBlpfIhPIS3",
        "outputId": "a216b72e-5150-48f9-9d78-d88b9a598e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "all_stopwords_sp = sp.Defaults.stop_words\n",
        "print(type(all_stopwords_sp))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'set'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42d0jUw2PSqJ",
        "outputId": "0bf12cdb-466a-400a-c47a-dec9310e9936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_stopwords_gn = gensim.parsing.preprocessing.STOPWORDS\n",
        "print(type(all_stopwords_gn))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'frozenset'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7oj_xJrP2w1",
        "outputId": "5779fba7-e7ca-457d-da44-b8f77fca8f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "all_stopwords=set(stopwords.words('english'))|all_stopwords_sp|all_stopwords_gn\n",
        "print(len(all_stopwords))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBnRUA-LZF6s"
      },
      "source": [
        "## Text cleansing functions defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfpfFPSYASei"
      },
      "source": [
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "#remove_stop_words()  \n",
        "def removeStopwords():\n",
        "  global text\n",
        "  sp = spacy.load('en_core_web_sm')\n",
        "  all_stopwords_sp = sp.Defaults.stop_words\n",
        "  all_stopwords_gn = gensim.parsing.preprocessing.STOPWORDS\n",
        "  all_stopwords_comb = all_stopwords_sp|all_stopwords_gn\n",
        "  text_tokens = word_tokenize(text)\n",
        "  #print('text_tokens',text_tokens)\n",
        "  tokens_without_sw= [word for word in text_tokens if not word in all_stopwords_comb]  \n",
        "  text = (\" \").join(tokens_without_sw)  \n",
        "\n",
        "def removeUnicode():\n",
        "    global text\n",
        "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n",
        "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r' ', text)       \n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
        "\n",
        "def replaceURL():\n",
        "    global text\n",
        "    \"\"\" Replaces url address with \"url\" \"\"\"\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
        "#    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',text)\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "    text = text.replace(\"-\",\"\").replace(\".com\",\"\")\n",
        "\n",
        "def replaceAtUser():\n",
        "    global text\n",
        "    \"\"\" Replaces \"@user\" with \"atUser\" \"\"\"\n",
        "    text = re.sub('@[^\\s]+',' ',text)\n",
        "\n",
        "def removeHashtagInFrontOfWord():\n",
        "    global text\n",
        "    \"\"\" Removes hastag in front of a word \"\"\"\n",
        "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
        "\n",
        "def removeNumbers():\n",
        "    global text\n",
        "    \"\"\" Removes integers \"\"\"\n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "\n",
        "def replaceMultiExclamationMark():\n",
        "    global text\n",
        "    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n",
        "#    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
        "    text = re.sub(r\"(\\!)\\1+\", ' ', text)\n",
        "\n",
        "def replaceMultiQuestionMark():\n",
        "    global text\n",
        "    \"\"\" Replaces repetitions of question marks \"\"\"\n",
        "#    text = re.sub(r\"(\\?)\\1+\", ' multiQuestion ', text)\n",
        "    text = re.sub(r\"(\\?)\\1+\", ' ', text)\n",
        "\n",
        "def replaceMultiStopMark():\n",
        "    global text\n",
        "    \"\"\" Replaces repetitions of stop marks \"\"\"\n",
        "#    text = re.sub(r\"(\\.)\\1+\", ' multiStop ', text)\n",
        "    text = re.sub(r\"(\\.)\\1+\", '.', text)\n",
        "    \n",
        "def remove_special_characters():\n",
        "    global text\n",
        "    '''\n",
        "    Function to clean the text in a tweet by removing \n",
        "    links and special characters with regex.\n",
        "    '''\n",
        "    text = ' '.join(re.sub(\"(@[A-Za-z[0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\/S+)\", \" \", text).split())\n",
        "    \n",
        "def tokenize_text():\n",
        "    global text\n",
        "    global tokens\n",
        "    tokens=text.split()\n",
        "#    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "def stemming_text():\n",
        "    global tokens\n",
        "    ps_stem = PorterStemmer()\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = ps_stem.stem(tokens[i])\n",
        "\n",
        "def lemmatize_text():\n",
        "    global text\n",
        "    global tokens\n",
        "    \n",
        "    tokens=text.split()\n",
        "    lemma = WordNetLemmatizer()\n",
        "    \n",
        "    for t in tokens:\n",
        "        t1 = lemma.lemmatize(t)\n",
        "        t1 = \" \" + str(t1) + \" \"\n",
        "        text=text.replace(t,t1)\n",
        "      \n",
        "def smart_lemmatize():\n",
        "        global text\n",
        "        global tokens\n",
        "        \n",
        "        \"\"\"Detects the part of speech for each word and then lemmatizes using that tag if available (default to Noun)\n",
        "       Returns\n",
        "       -------\n",
        "       list\n",
        "           list of lemmatized words\n",
        "       Examples\n",
        "       --------\n",
        "       >>> self.smart_lemmatize(['testing','this','out'])\n",
        "       ['test','this','out']\n",
        "        \"\"\"\n",
        "        new_sentence = []\n",
        "        lemma = WordNetLemmatizer()\n",
        "        #print(text)\n",
        "        text_split=text.split(\" \")\n",
        "\n",
        "        tagged_sentence = pos_tag([word for word in text_split if word])\n",
        "        #print(tagged_sentence)\n",
        "        for word, tag in tagged_sentence:\n",
        "            if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "                pos = 'n'\n",
        "            elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                pos = 'v'\n",
        "            elif tag in ['JJ', 'JJR', 'JJS']:\n",
        "                pos = 'a'\n",
        "            elif tag in ['RB', 'RBR', 'RBS']:\n",
        "                pos = 'r'\n",
        "            else:\n",
        "                pos = 'n'\n",
        "            lemma_word = lemma.lemmatize(word=word, pos=pos)\n",
        "            \n",
        "            new_sentence.append(lemma_word)\n",
        "\n",
        "        #text=new_sentence\n",
        "        text2=\"\"\n",
        "        for i in new_sentence:\n",
        "            text2 = text2 + \" \" + i\n",
        "        text=text2\n",
        "        #print(text)\n",
        "\n",
        "def remove_names():\n",
        "        global text,nlp\n",
        "        new_sentence = []\n",
        "        #print(text)        \n",
        "        doc=nlp(text)\n",
        "        for ent in doc.ents:\n",
        "          #print(ent.text,ent.label_)\n",
        "          text = text.replace(ent.text,\"\")\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlL9FVHzqQrU"
      },
      "source": [
        "## Text Preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W66qg437Iih6"
      },
      "source": [
        "def clean_text_ngram(text1):\n",
        "    global text\n",
        "    global tokens\n",
        "    \n",
        "    text = text1\n",
        "    remove_names()\n",
        "    text=text.lower()\n",
        "    removeStopwords()      \n",
        "    text=\" \" + text + \" \"\n",
        "    #stemming_text()\n",
        "    #lemmatize_text()\n",
        "    #replace_incomplete_word()\n",
        "    smart_lemmatize()\n",
        "    removeUnicode()\n",
        "    replaceURL()\n",
        "    replaceAtUser()\n",
        "    removeHashtagInFrontOfWord()\n",
        "    removeNumbers()    \n",
        "    replaceMultiExclamationMark()\n",
        "    replaceMultiQuestionMark()\n",
        "    replaceMultiStopMark()\n",
        "    remove_special_characters()\n",
        "    text1 = text   \n",
        "    return text1\n",
        "\n",
        "   \n",
        "def tokenize_text(df_test):    \n",
        "    global texts_cln,texts_mod\n",
        "\n",
        "    texts_cln=[]\n",
        "    texts_mod=[]\n",
        "    i=0\n",
        "    \n",
        "    for text1 in df_test.Complain_text:        \n",
        "        text1=clean_text_ngram(text1)\n",
        "        i=i+1\n",
        "        print(i)\n",
        "        s=StringIO()\n",
        "        s.write(text1)\n",
        "        k=s.getvalue()\n",
        "        s.close()\n",
        "        k=k.lower()\n",
        "        k=k.split()        \n",
        "        # Next only want valid strings\n",
        "        words = co.Counter(nltk.corpus.words.words())\n",
        "        k=[i for i in k if i in words]\n",
        "        s=\" \".join(k)\n",
        "        c = co.Counter(k)    \n",
        "        texts_mod.append(text1)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRQP-XqkqYog"
      },
      "source": [
        "## Cleansing input complaint text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlMxEIodNF9K",
        "outputId": "0d45f759-2424-4e54-f702-d802ea3ca9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_texts = df_texts.reset_index(drop=True)\n",
        "tokenize_text(df_texts)\n",
        "df_mod=pd.DataFrame(texts_mod,columns=['modified_text'])\n",
        "df_mod.index = df_texts.index\n",
        "df_all=df_texts\n",
        "df_all=df_all.join(df_mod)\n",
        "df_all = df_all[df_all['modified_text'].notnull() == True]\n",
        "df_all['COMPLAINT_ID'] = df_all.index\n",
        "df_all = df_all[['COMPLAINT_ID', 'Complain_text', 'modified_text']].rename(columns={'COMPLAINT_ID':'Complaint_ID','modified_text':'Modified_text'})\n",
        "title_name=\"CFPB_text_vs_cnsd_text\"\n",
        "dt_stamp = datetime.datetime.strftime(datetime.datetime.now(),'%Y%m%d_%H_%M_%S')\n",
        "file_name=str(title_name)+str(\"_\")+str(dt_stamp)+ str(\".xlsx\")\n",
        "df_product_and_complaint_reduced = df_product_and_complaint_reduced.join(df_all[['Modified_text']])\n",
        "df_product_and_complaint_reduced.to_excel(file_name, header=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n",
            "1258\n",
            "1259\n",
            "1260\n",
            "1261\n",
            "1262\n",
            "1263\n",
            "1264\n",
            "1265\n",
            "1266\n",
            "1267\n",
            "1268\n",
            "1269\n",
            "1270\n",
            "1271\n",
            "1272\n",
            "1273\n",
            "1274\n",
            "1275\n",
            "1276\n",
            "1277\n",
            "1278\n",
            "1279\n",
            "1280\n",
            "1281\n",
            "1282\n",
            "1283\n",
            "1284\n",
            "1285\n",
            "1286\n",
            "1287\n",
            "1288\n",
            "1289\n",
            "1290\n",
            "1291\n",
            "1292\n",
            "1293\n",
            "1294\n",
            "1295\n",
            "1296\n",
            "1297\n",
            "1298\n",
            "1299\n",
            "1300\n",
            "1301\n",
            "1302\n",
            "1303\n",
            "1304\n",
            "1305\n",
            "1306\n",
            "1307\n",
            "1308\n",
            "1309\n",
            "1310\n",
            "1311\n",
            "1312\n",
            "1313\n",
            "1314\n",
            "1315\n",
            "1316\n",
            "1317\n",
            "1318\n",
            "1319\n",
            "1320\n",
            "1321\n",
            "1322\n",
            "1323\n",
            "1324\n",
            "1325\n",
            "1326\n",
            "1327\n",
            "1328\n",
            "1329\n",
            "1330\n",
            "1331\n",
            "1332\n",
            "1333\n",
            "1334\n",
            "1335\n",
            "1336\n",
            "1337\n",
            "1338\n",
            "1339\n",
            "1340\n",
            "1341\n",
            "1342\n",
            "1343\n",
            "1344\n",
            "1345\n",
            "1346\n",
            "1347\n",
            "1348\n",
            "1349\n",
            "1350\n",
            "1351\n",
            "1352\n",
            "1353\n",
            "1354\n",
            "1355\n",
            "1356\n",
            "1357\n",
            "1358\n",
            "1359\n",
            "1360\n",
            "1361\n",
            "1362\n",
            "1363\n",
            "1364\n",
            "1365\n",
            "1366\n",
            "1367\n",
            "1368\n",
            "1369\n",
            "1370\n",
            "1371\n",
            "1372\n",
            "1373\n",
            "1374\n",
            "1375\n",
            "1376\n",
            "1377\n",
            "1378\n",
            "1379\n",
            "1380\n",
            "1381\n",
            "1382\n",
            "1383\n",
            "1384\n",
            "1385\n",
            "1386\n",
            "1387\n",
            "1388\n",
            "1389\n",
            "1390\n",
            "1391\n",
            "1392\n",
            "1393\n",
            "1394\n",
            "1395\n",
            "1396\n",
            "1397\n",
            "1398\n",
            "1399\n",
            "1400\n",
            "1401\n",
            "1402\n",
            "1403\n",
            "1404\n",
            "1405\n",
            "1406\n",
            "1407\n",
            "1408\n",
            "1409\n",
            "1410\n",
            "1411\n",
            "1412\n",
            "1413\n",
            "1414\n",
            "1415\n",
            "1416\n",
            "1417\n",
            "1418\n",
            "1419\n",
            "1420\n",
            "1421\n",
            "1422\n",
            "1423\n",
            "1424\n",
            "1425\n",
            "1426\n",
            "1427\n",
            "1428\n",
            "1429\n",
            "1430\n",
            "1431\n",
            "1432\n",
            "1433\n",
            "1434\n",
            "1435\n",
            "1436\n",
            "1437\n",
            "1438\n",
            "1439\n",
            "1440\n",
            "1441\n",
            "1442\n",
            "1443\n",
            "1444\n",
            "1445\n",
            "1446\n",
            "1447\n",
            "1448\n",
            "1449\n",
            "1450\n",
            "1451\n",
            "1452\n",
            "1453\n",
            "1454\n",
            "1455\n",
            "1456\n",
            "1457\n",
            "1458\n",
            "1459\n",
            "1460\n",
            "1461\n",
            "1462\n",
            "1463\n",
            "1464\n",
            "1465\n",
            "1466\n",
            "1467\n",
            "1468\n",
            "1469\n",
            "1470\n",
            "1471\n",
            "1472\n",
            "1473\n",
            "1474\n",
            "1475\n",
            "1476\n",
            "1477\n",
            "1478\n",
            "1479\n",
            "1480\n",
            "1481\n",
            "1482\n",
            "1483\n",
            "1484\n",
            "1485\n",
            "1486\n",
            "1487\n",
            "1488\n",
            "1489\n",
            "1490\n",
            "1491\n",
            "1492\n",
            "1493\n",
            "1494\n",
            "1495\n",
            "1496\n",
            "1497\n",
            "1498\n",
            "1499\n",
            "1500\n",
            "1501\n",
            "1502\n",
            "1503\n",
            "1504\n",
            "1505\n",
            "1506\n",
            "1507\n",
            "1508\n",
            "1509\n",
            "1510\n",
            "1511\n",
            "1512\n",
            "1513\n",
            "1514\n",
            "1515\n",
            "1516\n",
            "1517\n",
            "1518\n",
            "1519\n",
            "1520\n",
            "1521\n",
            "1522\n",
            "1523\n",
            "1524\n",
            "1525\n",
            "1526\n",
            "1527\n",
            "1528\n",
            "1529\n",
            "1530\n",
            "1531\n",
            "1532\n",
            "1533\n",
            "1534\n",
            "1535\n",
            "1536\n",
            "1537\n",
            "1538\n",
            "1539\n",
            "1540\n",
            "1541\n",
            "1542\n",
            "1543\n",
            "1544\n",
            "1545\n",
            "1546\n",
            "1547\n",
            "1548\n",
            "1549\n",
            "1550\n",
            "1551\n",
            "1552\n",
            "1553\n",
            "1554\n",
            "1555\n",
            "1556\n",
            "1557\n",
            "1558\n",
            "1559\n",
            "1560\n",
            "1561\n",
            "1562\n",
            "1563\n",
            "1564\n",
            "1565\n",
            "1566\n",
            "1567\n",
            "1568\n",
            "1569\n",
            "1570\n",
            "1571\n",
            "1572\n",
            "1573\n",
            "1574\n",
            "1575\n",
            "1576\n",
            "1577\n",
            "1578\n",
            "1579\n",
            "1580\n",
            "1581\n",
            "1582\n",
            "1583\n",
            "1584\n",
            "1585\n",
            "1586\n",
            "1587\n",
            "1588\n",
            "1589\n",
            "1590\n",
            "1591\n",
            "1592\n",
            "1593\n",
            "1594\n",
            "1595\n",
            "1596\n",
            "1597\n",
            "1598\n",
            "1599\n",
            "1600\n",
            "1601\n",
            "1602\n",
            "1603\n",
            "1604\n",
            "1605\n",
            "1606\n",
            "1607\n",
            "1608\n",
            "1609\n",
            "1610\n",
            "1611\n",
            "1612\n",
            "1613\n",
            "1614\n",
            "1615\n",
            "1616\n",
            "1617\n",
            "1618\n",
            "1619\n",
            "1620\n",
            "1621\n",
            "1622\n",
            "1623\n",
            "1624\n",
            "1625\n",
            "1626\n",
            "1627\n",
            "1628\n",
            "1629\n",
            "1630\n",
            "1631\n",
            "1632\n",
            "1633\n",
            "1634\n",
            "1635\n",
            "1636\n",
            "1637\n",
            "1638\n",
            "1639\n",
            "1640\n",
            "1641\n",
            "1642\n",
            "1643\n",
            "1644\n",
            "1645\n",
            "1646\n",
            "1647\n",
            "1648\n",
            "1649\n",
            "1650\n",
            "1651\n",
            "1652\n",
            "1653\n",
            "1654\n",
            "1655\n",
            "1656\n",
            "1657\n",
            "1658\n",
            "1659\n",
            "1660\n",
            "1661\n",
            "1662\n",
            "1663\n",
            "1664\n",
            "1665\n",
            "1666\n",
            "1667\n",
            "1668\n",
            "1669\n",
            "1670\n",
            "1671\n",
            "1672\n",
            "1673\n",
            "1674\n",
            "1675\n",
            "1676\n",
            "1677\n",
            "1678\n",
            "1679\n",
            "1680\n",
            "1681\n",
            "1682\n",
            "1683\n",
            "1684\n",
            "1685\n",
            "1686\n",
            "1687\n",
            "1688\n",
            "1689\n",
            "1690\n",
            "1691\n",
            "1692\n",
            "1693\n",
            "1694\n",
            "1695\n",
            "1696\n",
            "1697\n",
            "1698\n",
            "1699\n",
            "1700\n",
            "1701\n",
            "1702\n",
            "1703\n",
            "1704\n",
            "1705\n",
            "1706\n",
            "1707\n",
            "1708\n",
            "1709\n",
            "1710\n",
            "1711\n",
            "1712\n",
            "1713\n",
            "1714\n",
            "1715\n",
            "1716\n",
            "1717\n",
            "1718\n",
            "1719\n",
            "1720\n",
            "1721\n",
            "1722\n",
            "1723\n",
            "1724\n",
            "1725\n",
            "1726\n",
            "1727\n",
            "1728\n",
            "1729\n",
            "1730\n",
            "1731\n",
            "1732\n",
            "1733\n",
            "1734\n",
            "1735\n",
            "1736\n",
            "1737\n",
            "1738\n",
            "1739\n",
            "1740\n",
            "1741\n",
            "1742\n",
            "1743\n",
            "1744\n",
            "1745\n",
            "1746\n",
            "1747\n",
            "1748\n",
            "1749\n",
            "1750\n",
            "1751\n",
            "1752\n",
            "1753\n",
            "1754\n",
            "1755\n",
            "1756\n",
            "1757\n",
            "1758\n",
            "1759\n",
            "1760\n",
            "1761\n",
            "1762\n",
            "1763\n",
            "1764\n",
            "1765\n",
            "1766\n",
            "1767\n",
            "1768\n",
            "1769\n",
            "1770\n",
            "1771\n",
            "1772\n",
            "1773\n",
            "1774\n",
            "1775\n",
            "1776\n",
            "1777\n",
            "1778\n",
            "1779\n",
            "1780\n",
            "1781\n",
            "1782\n",
            "1783\n",
            "1784\n",
            "1785\n",
            "1786\n",
            "1787\n",
            "1788\n",
            "1789\n",
            "1790\n",
            "1791\n",
            "1792\n",
            "1793\n",
            "1794\n",
            "1795\n",
            "1796\n",
            "1797\n",
            "1798\n",
            "1799\n",
            "1800\n",
            "1801\n",
            "1802\n",
            "1803\n",
            "1804\n",
            "1805\n",
            "1806\n",
            "1807\n",
            "1808\n",
            "1809\n",
            "1810\n",
            "1811\n",
            "1812\n",
            "1813\n",
            "1814\n",
            "1815\n",
            "1816\n",
            "1817\n",
            "1818\n",
            "1819\n",
            "1820\n",
            "1821\n",
            "1822\n",
            "1823\n",
            "1824\n",
            "1825\n",
            "1826\n",
            "1827\n",
            "1828\n",
            "1829\n",
            "1830\n",
            "1831\n",
            "1832\n",
            "1833\n",
            "1834\n",
            "1835\n",
            "1836\n",
            "1837\n",
            "1838\n",
            "1839\n",
            "1840\n",
            "1841\n",
            "1842\n",
            "1843\n",
            "1844\n",
            "1845\n",
            "1846\n",
            "1847\n",
            "1848\n",
            "1849\n",
            "1850\n",
            "1851\n",
            "1852\n",
            "1853\n",
            "1854\n",
            "1855\n",
            "1856\n",
            "1857\n",
            "1858\n",
            "1859\n",
            "1860\n",
            "1861\n",
            "1862\n",
            "1863\n",
            "1864\n",
            "1865\n",
            "1866\n",
            "1867\n",
            "1868\n",
            "1869\n",
            "1870\n",
            "1871\n",
            "1872\n",
            "1873\n",
            "1874\n",
            "1875\n",
            "1876\n",
            "1877\n",
            "1878\n",
            "1879\n",
            "1880\n",
            "1881\n",
            "1882\n",
            "1883\n",
            "1884\n",
            "1885\n",
            "1886\n",
            "1887\n",
            "1888\n",
            "1889\n",
            "1890\n",
            "1891\n",
            "1892\n",
            "1893\n",
            "1894\n",
            "1895\n",
            "1896\n",
            "1897\n",
            "1898\n",
            "1899\n",
            "1900\n",
            "1901\n",
            "1902\n",
            "1903\n",
            "1904\n",
            "1905\n",
            "1906\n",
            "1907\n",
            "1908\n",
            "1909\n",
            "1910\n",
            "1911\n",
            "1912\n",
            "1913\n",
            "1914\n",
            "1915\n",
            "1916\n",
            "1917\n",
            "1918\n",
            "1919\n",
            "1920\n",
            "1921\n",
            "1922\n",
            "1923\n",
            "1924\n",
            "1925\n",
            "1926\n",
            "1927\n",
            "1928\n",
            "1929\n",
            "1930\n",
            "1931\n",
            "1932\n",
            "1933\n",
            "1934\n",
            "1935\n",
            "1936\n",
            "1937\n",
            "1938\n",
            "1939\n",
            "1940\n",
            "1941\n",
            "1942\n",
            "1943\n",
            "1944\n",
            "1945\n",
            "1946\n",
            "1947\n",
            "1948\n",
            "1949\n",
            "1950\n",
            "1951\n",
            "1952\n",
            "1953\n",
            "1954\n",
            "1955\n",
            "1956\n",
            "1957\n",
            "1958\n",
            "1959\n",
            "1960\n",
            "1961\n",
            "1962\n",
            "1963\n",
            "1964\n",
            "1965\n",
            "1966\n",
            "1967\n",
            "1968\n",
            "1969\n",
            "1970\n",
            "1971\n",
            "1972\n",
            "1973\n",
            "1974\n",
            "1975\n",
            "1976\n",
            "1977\n",
            "1978\n",
            "1979\n",
            "1980\n",
            "1981\n",
            "1982\n",
            "1983\n",
            "1984\n",
            "1985\n",
            "1986\n",
            "1987\n",
            "1988\n",
            "1989\n",
            "1990\n",
            "1991\n",
            "1992\n",
            "1993\n",
            "1994\n",
            "1995\n",
            "1996\n",
            "1997\n",
            "1998\n",
            "1999\n",
            "2000\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "2025\n",
            "2026\n",
            "2027\n",
            "2028\n",
            "2029\n",
            "2030\n",
            "2031\n",
            "2032\n",
            "2033\n",
            "2034\n",
            "2035\n",
            "2036\n",
            "2037\n",
            "2038\n",
            "2039\n",
            "2040\n",
            "2041\n",
            "2042\n",
            "2043\n",
            "2044\n",
            "2045\n",
            "2046\n",
            "2047\n",
            "2048\n",
            "2049\n",
            "2050\n",
            "2051\n",
            "2052\n",
            "2053\n",
            "2054\n",
            "2055\n",
            "2056\n",
            "2057\n",
            "2058\n",
            "2059\n",
            "2060\n",
            "2061\n",
            "2062\n",
            "2063\n",
            "2064\n",
            "2065\n",
            "2066\n",
            "2067\n",
            "2068\n",
            "2069\n",
            "2070\n",
            "2071\n",
            "2072\n",
            "2073\n",
            "2074\n",
            "2075\n",
            "2076\n",
            "2077\n",
            "2078\n",
            "2079\n",
            "2080\n",
            "2081\n",
            "2082\n",
            "2083\n",
            "2084\n",
            "2085\n",
            "2086\n",
            "2087\n",
            "2088\n",
            "2089\n",
            "2090\n",
            "2091\n",
            "2092\n",
            "2093\n",
            "2094\n",
            "2095\n",
            "2096\n",
            "2097\n",
            "2098\n",
            "2099\n",
            "2100\n",
            "2101\n",
            "2102\n",
            "2103\n",
            "2104\n",
            "2105\n",
            "2106\n",
            "2107\n",
            "2108\n",
            "2109\n",
            "2110\n",
            "2111\n",
            "2112\n",
            "2113\n",
            "2114\n",
            "2115\n",
            "2116\n",
            "2117\n",
            "2118\n",
            "2119\n",
            "2120\n",
            "2121\n",
            "2122\n",
            "2123\n",
            "2124\n",
            "2125\n",
            "2126\n",
            "2127\n",
            "2128\n",
            "2129\n",
            "2130\n",
            "2131\n",
            "2132\n",
            "2133\n",
            "2134\n",
            "2135\n",
            "2136\n",
            "2137\n",
            "2138\n",
            "2139\n",
            "2140\n",
            "2141\n",
            "2142\n",
            "2143\n",
            "2144\n",
            "2145\n",
            "2146\n",
            "2147\n",
            "2148\n",
            "2149\n",
            "2150\n",
            "2151\n",
            "2152\n",
            "2153\n",
            "2154\n",
            "2155\n",
            "2156\n",
            "2157\n",
            "2158\n",
            "2159\n",
            "2160\n",
            "2161\n",
            "2162\n",
            "2163\n",
            "2164\n",
            "2165\n",
            "2166\n",
            "2167\n",
            "2168\n",
            "2169\n",
            "2170\n",
            "2171\n",
            "2172\n",
            "2173\n",
            "2174\n",
            "2175\n",
            "2176\n",
            "2177\n",
            "2178\n",
            "2179\n",
            "2180\n",
            "2181\n",
            "2182\n",
            "2183\n",
            "2184\n",
            "2185\n",
            "2186\n",
            "2187\n",
            "2188\n",
            "2189\n",
            "2190\n",
            "2191\n",
            "2192\n",
            "2193\n",
            "2194\n",
            "2195\n",
            "2196\n",
            "2197\n",
            "2198\n",
            "2199\n",
            "2200\n",
            "2201\n",
            "2202\n",
            "2203\n",
            "2204\n",
            "2205\n",
            "2206\n",
            "2207\n",
            "2208\n",
            "2209\n",
            "2210\n",
            "2211\n",
            "2212\n",
            "2213\n",
            "2214\n",
            "2215\n",
            "2216\n",
            "2217\n",
            "2218\n",
            "2219\n",
            "2220\n",
            "2221\n",
            "2222\n",
            "2223\n",
            "2224\n",
            "2225\n",
            "2226\n",
            "2227\n",
            "2228\n",
            "2229\n",
            "2230\n",
            "2231\n",
            "2232\n",
            "2233\n",
            "2234\n",
            "2235\n",
            "2236\n",
            "2237\n",
            "2238\n",
            "2239\n",
            "2240\n",
            "2241\n",
            "2242\n",
            "2243\n",
            "2244\n",
            "2245\n",
            "2246\n",
            "2247\n",
            "2248\n",
            "2249\n",
            "2250\n",
            "2251\n",
            "2252\n",
            "2253\n",
            "2254\n",
            "2255\n",
            "2256\n",
            "2257\n",
            "2258\n",
            "2259\n",
            "2260\n",
            "2261\n",
            "2262\n",
            "2263\n",
            "2264\n",
            "2265\n",
            "2266\n",
            "2267\n",
            "2268\n",
            "2269\n",
            "2270\n",
            "2271\n",
            "2272\n",
            "2273\n",
            "2274\n",
            "2275\n",
            "2276\n",
            "2277\n",
            "2278\n",
            "2279\n",
            "2280\n",
            "2281\n",
            "2282\n",
            "2283\n",
            "2284\n",
            "2285\n",
            "2286\n",
            "2287\n",
            "2288\n",
            "2289\n",
            "2290\n",
            "2291\n",
            "2292\n",
            "2293\n",
            "2294\n",
            "2295\n",
            "2296\n",
            "2297\n",
            "2298\n",
            "2299\n",
            "2300\n",
            "2301\n",
            "2302\n",
            "2303\n",
            "2304\n",
            "2305\n",
            "2306\n",
            "2307\n",
            "2308\n",
            "2309\n",
            "2310\n",
            "2311\n",
            "2312\n",
            "2313\n",
            "2314\n",
            "2315\n",
            "2316\n",
            "2317\n",
            "2318\n",
            "2319\n",
            "2320\n",
            "2321\n",
            "2322\n",
            "2323\n",
            "2324\n",
            "2325\n",
            "2326\n",
            "2327\n",
            "2328\n",
            "2329\n",
            "2330\n",
            "2331\n",
            "2332\n",
            "2333\n",
            "2334\n",
            "2335\n",
            "2336\n",
            "2337\n",
            "2338\n",
            "2339\n",
            "2340\n",
            "2341\n",
            "2342\n",
            "2343\n",
            "2344\n",
            "2345\n",
            "2346\n",
            "2347\n",
            "2348\n",
            "2349\n",
            "2350\n",
            "2351\n",
            "2352\n",
            "2353\n",
            "2354\n",
            "2355\n",
            "2356\n",
            "2357\n",
            "2358\n",
            "2359\n",
            "2360\n",
            "2361\n",
            "2362\n",
            "2363\n",
            "2364\n",
            "2365\n",
            "2366\n",
            "2367\n",
            "2368\n",
            "2369\n",
            "2370\n",
            "2371\n",
            "2372\n",
            "2373\n",
            "2374\n",
            "2375\n",
            "2376\n",
            "2377\n",
            "2378\n",
            "2379\n",
            "2380\n",
            "2381\n",
            "2382\n",
            "2383\n",
            "2384\n",
            "2385\n",
            "2386\n",
            "2387\n",
            "2388\n",
            "2389\n",
            "2390\n",
            "2391\n",
            "2392\n",
            "2393\n",
            "2394\n",
            "2395\n",
            "2396\n",
            "2397\n",
            "2398\n",
            "2399\n",
            "2400\n",
            "2401\n",
            "2402\n",
            "2403\n",
            "2404\n",
            "2405\n",
            "2406\n",
            "2407\n",
            "2408\n",
            "2409\n",
            "2410\n",
            "2411\n",
            "2412\n",
            "2413\n",
            "2414\n",
            "2415\n",
            "2416\n",
            "2417\n",
            "2418\n",
            "2419\n",
            "2420\n",
            "2421\n",
            "2422\n",
            "2423\n",
            "2424\n",
            "2425\n",
            "2426\n",
            "2427\n",
            "2428\n",
            "2429\n",
            "2430\n",
            "2431\n",
            "2432\n",
            "2433\n",
            "2434\n",
            "2435\n",
            "2436\n",
            "2437\n",
            "2438\n",
            "2439\n",
            "2440\n",
            "2441\n",
            "2442\n",
            "2443\n",
            "2444\n",
            "2445\n",
            "2446\n",
            "2447\n",
            "2448\n",
            "2449\n",
            "2450\n",
            "2451\n",
            "2452\n",
            "2453\n",
            "2454\n",
            "2455\n",
            "2456\n",
            "2457\n",
            "2458\n",
            "2459\n",
            "2460\n",
            "2461\n",
            "2462\n",
            "2463\n",
            "2464\n",
            "2465\n",
            "2466\n",
            "2467\n",
            "2468\n",
            "2469\n",
            "2470\n",
            "2471\n",
            "2472\n",
            "2473\n",
            "2474\n",
            "2475\n",
            "2476\n",
            "2477\n",
            "2478\n",
            "2479\n",
            "2480\n",
            "2481\n",
            "2482\n",
            "2483\n",
            "2484\n",
            "2485\n",
            "2486\n",
            "2487\n",
            "2488\n",
            "2489\n",
            "2490\n",
            "2491\n",
            "2492\n",
            "2493\n",
            "2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij5xUBoUrfBg"
      },
      "source": [
        "## Product level data distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKA9mvRZQI9Q",
        "outputId": "873bc4c2-a4e6-4824-a6a7-dd0af61c43c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = df_product_and_complaint_reduced[['PRODUCT','ISSUE','COMPLAINT_ID','Modified_text']].rename(columns={'Modified_text':'CONSUMER_COMPLAINT'})\n",
        "print(df[['PRODUCT']].value_counts(normalize=True)*100)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PRODUCT                    \n",
            "Debt collection                27.663230\n",
            "Mortgage                       16.247590\n",
            "Credit card                    13.846283\n",
            "Loan                           12.610008\n",
            "Prepaid card                    8.243232\n",
            "Credit reporting                8.176180\n",
            "Checking or savings account     5.183975\n",
            "Bank account or service         3.838739\n",
            "Money transfers                 2.661135\n",
            "Other financial service         1.529629\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSIQ6skqnxuq"
      },
      "source": [
        "### Section 1.1.2: Subsetting Dataframe for Text Multi-Classification Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a2Ns03onxuu"
      },
      "source": [
        "# Subsetting dataframe into columns useful for our text multi-classification problem\n",
        "df = df[['PRODUCT','CONSUMER_COMPLAINT']].copy()\n",
        "df.CONSUMER_COMPLAINT = df.CONSUMER_COMPLAINT.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M74PR5PGnxu5"
      },
      "source": [
        "# Pickling our subsetted dataframe\n",
        "with open('df_data.pickle', 'wb') as to_write:\n",
        "     pickle.dump(df, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qmi3sGonxvH"
      },
      "source": [
        "# Loading our pickled subsetted dataframe\n",
        "with open('df_data.pickle', 'rb') as to_read:\n",
        "    df_dc = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9l9jMMmWARp"
      },
      "source": [
        "### Section 2.2.4: Label Encoding for Issue Categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l-PsGnkWARs"
      },
      "source": [
        "# Applying encoding to the ISSUE column\n",
        "df['PRODUCT_ID'] = df['PRODUCT'].factorize()[0] \n",
        "\n",
        "#.factorize[0] arranges the index of each encoded number accordingly to the \n",
        "# index of your categorical variables in the ISSUE column\n",
        "\n",
        "# Creates a dataframe of the ISSUE to their respective ISSUE_ID\n",
        "category_id_df = df[['PRODUCT', 'PRODUCT_ID']].drop_duplicates()\n",
        "\n",
        "# Dictionaries for future use. Creating our cheatsheets for what each encoded label represents.\n",
        "category_to_id = dict(category_id_df.values) # Creates a ISSUE: ISSUE_ID key-value pair\n",
        "id_to_category = dict(category_id_df[['PRODUCT_ID', 'PRODUCT']].values)  # Creates a ISSUE_ID: ISSUE key-value pair\n",
        "\n",
        "# New dataframe\n",
        "df_dc.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8guO3OHWASf"
      },
      "source": [
        "Ok so basically, we can trust the stop_word removal from tfidf.   \n",
        "Through the tests, we see that it removes punctuations and also all the stopwords.   \n",
        "(I deleted these because they were seriously taking up a lot of space on my jupyter notebook)  \n",
        "Spellchecker might also mess up some words, especially with bank names like Citi for example  \n",
        "Thus changing the semantic meaning of the word.  \n",
        "  \n",
        "Therefore, we are going to just use tfidf just as it is.  \n",
        "It's pretty amazing, really.  \n",
        "  \n",
        "Great! We can start building some shit!  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kd590z7WASh"
      },
      "source": [
        "# Section 3: Model/Classifier Selection\n",
        "#### Each model is cross-validated on a stratified 5-fold split train-validation (of 80% of the whole dataset). The remaining 20% data was held out and untouched to simulate how our final chosen model would perform in real-life scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akm29_KaWASi"
      },
      "source": [
        "## Section 3.1: Train/StratifiedCV/Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW38XTiuWASj"
      },
      "source": [
        "In this section, we also prepare the model for text pre-processing. This is handled by our handydandy Tfidfvecotirzer with the stop_words and such. As we have tested above, it also ignores punctuation so it really is quite a catch-all package that cleans our text really well for the purposes of doing text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6hBJHSwWASp"
      },
      "source": [
        "# Split the data into X and y data sets\n",
        "X, y = df.CONSUMER_COMPLAINT, df.PRODUCT_ID\n",
        "print('X shape:', X.shape, 'y shape:', y.shape)\n",
        "\n",
        "# For text classification, ALWAYS split data first before vectorizing.\n",
        "# This is because you don't want to cheat by having features (words) from the test data already being inside your train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
        "                                                            test_size=0.2,   # 80% train/cv, 20% test\n",
        "                                                            stratify=y,\n",
        "                                                            random_state=seed)\n",
        "print('X_train', X_train_val.shape)\n",
        "print('y_train', y_train_val.shape)\n",
        "print('X_test', X_test.shape)\n",
        "print('y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUesUtG6-O1U"
      },
      "source": [
        "X.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPjqjI9b1UPF"
      },
      "source": [
        "## Section 2.2: Vectorization\n",
        "#### To turn words into something a machine learning algorithm can understand and process, we need to do something called vectorization.\n",
        "##### Put simply, this is the process of turning words into multi-dimensional vectors in such a way that the meaning or context of the word is correlated with where that vector points. In a sense, vectorization allows computers to quantify the meaning of words by mapping similar word meanings to similar vector spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOJlSmlgWAS7"
      },
      "source": [
        "# Performing Text Pre-Processing\n",
        "\n",
        "# Import tfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# The text needs to be transformed to vectors so as the algorithms will be able make predictions. \n",
        "# In this case it will be used the Term Frequency – Inverse Document Frequency (TFIDF) weight \n",
        "# to evaluate how important A WORD is to A DOCUMENT in a COLLECTION OF DOCUMENTS.\n",
        "\n",
        "# tfidf1 = 1-gram only. \n",
        "tfidf1 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                        min_df=5,\n",
        "                        stop_words='english')\n",
        "\n",
        "X_train_val_tfidf1 = tfidf1.fit_transform(X_train_val).toarray()\n",
        "X_test_tfidf1 = tfidf1.transform(X_test)\n",
        "\n",
        "# tfidf2 = unigram and bigram\n",
        "tfidf2 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                        min_df=5, \n",
        "                        ngram_range=(1,2), # we consider unigrams and bigrams\n",
        "                        stop_words='english')\n",
        "X_train_val_tfidf2 = tfidf2.fit_transform(X_train_val).toarray()\n",
        "X_test_tfidf2 = tfidf2.transform(X_test)\n",
        "\n",
        "# # StratifiedKFold -> Split 5\n",
        "# ## We now want to do stratified kfold to preserve the proportion of the category imbalances \n",
        "# # (number is split evenly from all the classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRA0gBIX1kzl"
      },
      "source": [
        "## Term Frequency — Inverse Document Frequency Vectorization\n",
        "#### For this application, we want context-based vectorization. This is where Term frequency inverse document frequency vectorization comes in (TF-IDF). This vectorization method looks at the number of times a word appears in a comment relative to the number of times it appears in other comments. Two things result in a higher TF-IDF score:\n",
        "\n",
        "   1. Higher frequency of the word within the specific complaint being scored.\n",
        "   2. Lower frequency of the word across all other complaints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfWY7uvIWATF"
      },
      "source": [
        "## Section 3.2: Baseline Model - Train/Stratified CV with MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAGt4GmEWATH"
      },
      "source": [
        "print('1-gram number of (rows, features):', X_train_val_tfidf1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjnQFVzs8UvL"
      },
      "source": [
        "### Evaluation Metrics (Accuracy, Recall_macro, Precission_macro, f1_macro, Recall_weighted, Precission_weighted, f1_weighted)\n",
        "Referrence:\n",
        "https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2\n",
        "\n",
        "https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gop5ZHYnWATS"
      },
      "source": [
        "def metric_cv_stratified(model, X_train_val, y_train_val, n_splits, name):\n",
        "    \"\"\"\n",
        "    Accepts a Model Object, converted X_train_val and y_train_val, n_splits, name\n",
        "    and returns a dataframe with various cross-validated metric scores \n",
        "    over a stratified n_splits kfold for a multi-class classifier.\n",
        "    \"\"\"\n",
        "    # Start timer\n",
        "    import timeit\n",
        "    start = timeit.default_timer()\n",
        "    \n",
        "    ### Computations below\n",
        "    \n",
        "    # StratifiedKFold\n",
        "    ## We now want to do stratified kfold to preserve the proportion of the category imbalances \n",
        "    # (number is split evenly from all the classes)\n",
        "    from sklearn.model_selection import StratifiedKFold  # incase user forgest to import\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    \n",
        "    # Initializing Metrics\n",
        "    accuracy = 0.0\n",
        "    micro_f1 = 0.0\n",
        "    macro_precision = 0.0\n",
        "    macro_recall = 0.0\n",
        "    macro_f1 = 0.0\n",
        "    weighted_precision = 0.0\n",
        "    weighted_recall = 0.0\n",
        "    weighted_f1 = 0.0\n",
        "#     roc_auc = 0.0    Not considering this score in this case\n",
        "        \n",
        "    # Storing metrics\n",
        "    from sklearn.model_selection import cross_val_score  # incase user forgets to import\n",
        "    accuracy = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='accuracy'))\n",
        "#     micro_f1 = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='f1_micro'))\n",
        "    macro_precision = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='precision_macro'))\n",
        "    macro_recall = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='recall_macro'))\n",
        "    macro_f1 = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='f1_macro'))\n",
        "    weighted_precision = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='precision_weighted'))\n",
        "    weighted_recall = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='recall_weighted'))\n",
        "    weighted_f1 = np.mean(cross_val_score(model, X_train_val, y_train_val, cv=kf, scoring='f1_weighted'))\n",
        "    \n",
        "    # Stop timer\n",
        "    stop = timeit.default_timer()\n",
        "    elapsed_time = stop - start\n",
        "    \n",
        "    return pd.DataFrame({'Model'    : [name],\n",
        "                         'Accuracy' : [accuracy],\n",
        "#                          'Micro F1' : [micro_f1],\n",
        "                         'Macro Precision': [macro_precision],\n",
        "                         'Macro Recall'   : [macro_recall],\n",
        "                         'Macro F1score'  : [macro_f1],\n",
        "                         'Weighted Precision': [weighted_precision],\n",
        "                         'Weighted Recall'   : [weighted_recall],\n",
        "                         'Weighted F1'  : [weighted_f1],\n",
        "                         'Time taken': [elapsed_time]  # timetaken: to be used for comparison later\n",
        "                        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNr3Tm2vWATv"
      },
      "source": [
        "## Section 3.3: Further Train/Straitified CV with other Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vtVo-OLWATx"
      },
      "source": [
        "### Section 3.3.1: 1-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSMvSkOGWATy"
      },
      "source": [
        "## Testing on all Models using 1-gram \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "dtc = DecisionTreeClassifier(max_depth = 10, random_state = 20)\n",
        "pac = PassiveAggressiveClassifier()\n",
        "xgb = XGBClassifier(max_depth = 7, n_estimators = 50)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_1gram = pd.concat([metric_cv_stratified(mnb, X_train_val_tfidf1, y_train_val, 5, 'MultinomialNB1'),\n",
        "                                           metric_cv_stratified(gnb, X_train_val_tfidf1, y_train_val, 5, 'GaussianNB1'),\n",
        "                                           metric_cv_stratified(logit, X_train_val_tfidf1, y_train_val, 5, 'LogisticRegression1'),\n",
        "                                           metric_cv_stratified(randomforest, X_train_val_tfidf1, y_train_val, 5, 'RandomForest1'),\n",
        "                                           metric_cv_stratified(dtc, X_train_val_tfidf1, y_train_val, 5, 'DecisionTreeClassifier1'),\n",
        "                                           metric_cv_stratified(pac, X_train_val_tfidf1, y_train_val, 5, 'PassiveAggressiveClassifier1'),\n",
        "                                           metric_cv_stratified(xgb, X_train_val_tfidf1, y_train_val, 5, 'XGBClassifier1'),\n",
        "                                           metric_cv_stratified(linearsvc, X_train_val_tfidf1, y_train_val, 5, 'LinearSVC1')\n",
        "                                          ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM-8TDPXWAT5"
      },
      "source": [
        "results_cv_straitified_1gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNTjuwAO2nNp"
      },
      "source": [
        "### Logistic Regression gives us the highest accuracy of 39%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyggSDCPWAT-"
      },
      "source": [
        "# Saving our results because I don't wanna re-train the whole damn thing\n",
        "with open('results_cv_straitified_1gram_df.pickle', 'wb') as to_write:\n",
        "  pickle.dump(results_cv_straitified_1gram, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ccp2S4WAUG"
      },
      "source": [
        "# Loading our pickled results\n",
        "with open('results_cv_straitified_1gram_df.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_1gram = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-8xfZSHWAUO"
      },
      "source": [
        "### Section 3.3.2: 2-gram (Keep running into memory issues, therefore, this will not be tested anymore)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6jIZ8tWAUP"
      },
      "source": [
        "### Keep running into memory issues with 2-gram. Therefore, will not test it anymore\n",
        "\n",
        "'''# Testing on all Models using 2-gram \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "knn = KNeighborsClassifier()\n",
        "decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "# # to concat all models\n",
        "results_cv_straitified_2gram = pd.concat([metric_cv_stratified(mnb, X_train_val_tfidf2, y_train_val, 5, 'MultinomialNB2'),\n",
        "                                          metric_cv_stratified(gnb, X_train_val_tfidf2, y_train_val, 5, 'GaussianNB2'),\n",
        "                                          metric_cv_stratified(logit, X_train_val_tfidf2, y_train_val, 5, 'LogisticRegression2'),\n",
        "                                          metric_cv_stratified(randomforest, X_train_val_tfidf2, y_train_val, 5, 'RandomForest2'),\n",
        "                                          metric_cv_stratified(dtc, X_train_val_tfidf2, y_train_val, 5, 'DecisionTreeClassifier'),\n",
        "                                          metric_cv_stratified(pac, X_train_val_tfidf2, y_train_val, 5, 'PassiveAggressiveClassifier'),\n",
        "                                          metric_cv_stratified(xgb, X_train_val_tfidf2, y_train_val, 5, 'XGBClassifier'),\n",
        "                                          metric_cv_stratified(linearsvc, X_train_val_tfidf2, y_train_val, 5, 'LinearSVC2')\n",
        "                                          ], axis=0).reset_index()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzeTQ9I0WAUX"
      },
      "source": [
        "#results_cv_straitified_2gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGe8Wlq520p9"
      },
      "source": [
        "Instead of utilizing Vectorization (Tf-idfVectorizer that we used above), we can utilize WordEmbedding on the complaints before feeding them into our classifiers.\n",
        "https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b\n",
        "\n",
        "https://towardsdatascience.com/the-three-main-branches-of-word-embeddings-7b90fa36dfb9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVGLoFG9WAUc"
      },
      "source": [
        "### Section 3.3.3: Using GloVe50d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekd6xGwNWAUd"
      },
      "source": [
        "Each complaint is mapped to a feature vector by averaging the word embeddings of all words in the review. These features are then fed into the defined function above for train/cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ImvfRPeWAUf"
      },
      "source": [
        "## Using pre-trained GloVe 'glove.6B.50d'\n",
        "\n",
        "glove_file = glove_dir = 'glove.6B.50d.txt'\n",
        "w2v_output_file = 'glove.6B.50d.txt.w2v'\n",
        "\n",
        "# The following utility converts file formats\n",
        "gensim.scripts.glove2word2vec.glove2word2vec(glove_file, w2v_output_file)\n",
        "\n",
        "# Now we can load it!\n",
        "glove_model_50d = gensim.models.KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)\n",
        "\n",
        "# Pickle glove model so we don't have to do the above steps again and keep the damn glove.6b.50d in our folder\n",
        "with open('glove_model_50d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(glove_model_50d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpbudUt1WAUj"
      },
      "source": [
        "# Load pickled glove_model\n",
        "with open('glove_model_50d.pickle', 'rb') as to_read:\n",
        "    glove_model_50d = pickle.load(to_read)\n",
        "    \n",
        "num_features = 50 # depends on the pre-trained model you are loading"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-XVT5xWAUo"
      },
      "source": [
        "def complaint_to_wordlist(review, remove_stopwords=False):\n",
        "    \"\"\"\n",
        "    Convert a complaint to a list of words. Removal of stop words is optional.\n",
        "    \"\"\"\n",
        "    # remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
        "    \n",
        "    # convert to lower case and split at whitespace\n",
        "    words = review_text.lower().split()\n",
        "    \n",
        "    # remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    return words    # list of tokenized and cleaned words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8yYLMcWAUu"
      },
      "source": [
        "# num_features refer to the dimensionality of the model you are using\n",
        "# model refers to the trained word2vec/glove model\n",
        "# words refer to the words in a single document/entry\n",
        "\n",
        "def make_feature_vec(words, model, num_features):\n",
        "    \"\"\"\n",
        "    Average the word vectors for a set of words\n",
        "    \"\"\"\n",
        "    feature_vec = np.zeros((num_features,),  # creates a zero matrix of (num_features, )\n",
        "                           dtype=\"float32\")  # pre-initialize (for speed)\n",
        "    \n",
        "    # Initialize a counter for the number of words in a complaint\n",
        "    nwords = 0.\n",
        "    index2word_set = set(model.index2word)  # words known to the model\n",
        "\n",
        "    \n",
        "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
        "    for word in words:   # for each word in the list of words\n",
        "        if word in index2word_set:   # if each word is found in the words known to the model\n",
        "            nwords = nwords + 1.     # add 1 to nwords\n",
        "            feature_vec = np.add(feature_vec, model[word])   \n",
        "    \n",
        "    # Divide by the number of words to get the average \n",
        "    if nwords > 0:\n",
        "        feature_vec = np.divide(feature_vec, nwords)\n",
        "    \n",
        "    return feature_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cFE0EfyWAU0"
      },
      "source": [
        "# complaints refers to the whole corpus you intend to put in. \n",
        "# Therefore you need to append all these info from your df into a list first\n",
        "\n",
        "def get_avg_feature_vecs(complaints, model, num_features):\n",
        "    \"\"\"\n",
        "    Calculate average feature vectors for ALL complaints\n",
        "    \"\"\"\n",
        "    # Initialize a counter for indexing \n",
        "    counter = 0\n",
        "    \n",
        "    # pre-initialize (for speed)\n",
        "    complaint_feature_vecs = np.zeros((len(complaints),num_features), dtype='float32')  \n",
        "    \n",
        "    for complaint in complaints: # each complaint is made up of tokenized/cleaned/stopwords removed words\n",
        "        complaint_feature_vecs[counter] = make_feature_vec(complaint, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return complaint_feature_vecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1BMWut2WAU5"
      },
      "source": [
        "# Tokenizing and vectorizing our Train_Val Complaints (80%)\n",
        "clean_train_val_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_train_val_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_train_val_glove_features = get_avg_feature_vecs(clean_train_val_complaints, glove_model_50d, num_features)\n",
        "\n",
        "# Tokenizing and vectorizing our Test Complaints (20%)\n",
        "clean_test_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_test_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_test_glove_features = get_avg_feature_vecs(clean_test_complaints, glove_model_50d, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LgsMquwWAU-"
      },
      "source": [
        "## Run the X_train_val_word2vec_features into our defined function for scoring \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_glove50d = pd.concat([\n",
        "#     metric_cv_stratified(mnb, X_train_val_glove_features, y_train_val, 5, 'MultinomialNB_glove50d'),\n",
        "     metric_cv_stratified(gnb, X_train_val_glove_features, y_train_val, 5, 'GaussianNB_glove50d'),\n",
        "     metric_cv_stratified(logit, X_train_val_glove_features, y_train_val, 5, 'LogisticRegression_glove50d'),\n",
        "     metric_cv_stratified(randomforest, X_train_val_glove_features, y_train_val, 5, 'RandomForest_glove50d'),\n",
        "     metric_cv_stratified(linearsvc, X_train_val_glove_features, y_train_val, 5, 'LinearSVC_glove50d')\n",
        "    ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLra8YW4WAVE"
      },
      "source": [
        "# Saving Results into a DF\n",
        "with open('results_cv_straitified_glove50d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_cv_straitified_glove50d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDbBTjhY2gv4"
      },
      "source": [
        "files.download('results_cv_straitified_glove50d.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g4V9CvNWAVJ"
      },
      "source": [
        "# Opening Results\n",
        "with open('results_cv_straitified_glove50d.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_glove50d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM2yVII5WAVO"
      },
      "source": [
        "results_cv_straitified_glove50d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF3Ngl0lWAVW"
      },
      "source": [
        "### Section 3.3.4: Using GloVe100d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnBd9ODiWAVY"
      },
      "source": [
        "del glove_model_50d, results_cv_straitified_glove50d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQOogP2tWAVd"
      },
      "source": [
        "## Using pre-trained GloVe\n",
        "# download from https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "num_features = 100 # depends on the pre-trained model you are loading\n",
        "\n",
        "glove_file = glove_dir = 'glove.6B.' + str(num_features) + 'd.txt'\n",
        "w2v_output_file = 'glove.6B.' + str(num_features) + 'd.txt.w2v'\n",
        "\n",
        "# The following utility converts file formats\n",
        "gensim.scripts.glove2word2vec.glove2word2vec(glove_file, w2v_output_file)\n",
        "\n",
        "# Now we can load it!\n",
        "glove_model_100d = gensim.models.KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)\n",
        "\n",
        "# Pickle glove model so we don't have to do the above steps again and keep the damn glove.6b.50d in our folder\n",
        "with open('glove_model_' + str(num_features) + 'd.pickle', 'wb') as to_write:\n",
        "    pickle.dump(glove_model_100d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx1Hf09Y2wZa"
      },
      "source": [
        "files.download('glove_model_' + str(num_features) + 'd.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_B6SSaXWAVp"
      },
      "source": [
        "# Load pickled glove_model\n",
        "with open('glove_model_100d.pickle', 'rb') as to_read:\n",
        "    glove_model_100d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONLbysHAWAVx"
      },
      "source": [
        "# For Train_Val Complaints (80%)\n",
        "clean_train_val_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_train_val_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_train_val_glove_features = get_avg_feature_vecs(clean_train_val_complaints, glove_model_100d, num_features)\n",
        "\n",
        "# For Test Complaints (20%)\n",
        "clean_test_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_test_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_test_glove_features = get_avg_feature_vecs(clean_test_complaints, glove_model_100d, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfwXcd0SWAV2"
      },
      "source": [
        "## Run the X_train_val_word2vec_features into our defined function for scoring \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_glove100d = pd.concat([\n",
        "#     metric_cv_stratified(mnb, X_train_val_glove_features, y_train_val, 5, 'MultinomialNB_glove50d'),\n",
        "     metric_cv_stratified(gnb, X_train_val_glove_features, y_train_val, 5, 'GaussianNB_glove100d'),\n",
        "     metric_cv_stratified(logit, X_train_val_glove_features, y_train_val, 5, 'LogisticRegression_glove100d'),\n",
        "     metric_cv_stratified(randomforest, X_train_val_glove_features, y_train_val, 5, 'RandomForest_glove100d'),\n",
        "     metric_cv_stratified(linearsvc, X_train_val_glove_features, y_train_val, 5, 'LinearSVC_glove100d')\n",
        "    ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTG-3B4NWAV9"
      },
      "source": [
        "with open('results_cv_straitified_glove100d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_cv_straitified_glove100d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZhT5z9W2yXW"
      },
      "source": [
        "files.download('results_cv_straitified_glove100d.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZin6a1mWAWB"
      },
      "source": [
        "# Opening Results\n",
        "with open('results_cv_straitified_glove100d.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_glove100d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0OgF7-tWAWG"
      },
      "source": [
        "results_cv_straitified_glove100d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZqRaWg5WAWM"
      },
      "source": [
        "### Section 3.3.5: Using GloVe200d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpaDRgppWAWN"
      },
      "source": [
        "del glove_model_100d, results_cv_straitified_glove100d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V6vxVD2WAWR"
      },
      "source": [
        "## Using pre-trained GloVe\n",
        "# download from https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "num_features = 200 # depends on the pre-trained model you are loading\n",
        "\n",
        "glove_file = glove_dir = 'glove.6B.' + str(num_features) + 'd.txt'\n",
        "w2v_output_file = 'glove.6B.' + str(num_features) + 'd.txt.w2v'\n",
        "\n",
        "# The following utility converts file formats\n",
        "gensim.scripts.glove2word2vec.glove2word2vec(glove_file, w2v_output_file)\n",
        "\n",
        "# Now we can load it!\n",
        "glove_model_200d = gensim.models.KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)\n",
        "\n",
        "# Pickle glove model so we don't have to do the above steps again and keep the damn glove.6b.50d in our folder\n",
        "with open('glove_model_' + str(num_features) + 'd.pickle', 'wb') as to_write:\n",
        "    pickle.dump(glove_model_200d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwne6nMpWAWY"
      },
      "source": [
        "# Load pickled glove_model\n",
        "with open('glove_model_200d.pickle', 'rb') as to_read:\n",
        "    glove_model_200d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nk2v4DD2_f7"
      },
      "source": [
        "files.download('glove_model_' + str(num_features) + 'd.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdk149U4WAWd"
      },
      "source": [
        "# For Train_Val Complaints (80%)\n",
        "clean_train_val_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_train_val_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_train_val_glove_features = get_avg_feature_vecs(clean_train_val_complaints, glove_model_200d, num_features)\n",
        "\n",
        "# Already run above\n",
        "# # For Test Complaints (20%)\n",
        "# clean_test_complaints = []\n",
        "# for complaint in X_train_val:\n",
        "#     clean_test_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "# X_test_glove_features = get_avg_feature_vecs(clean_test_complaints, glove_model_200d, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwxiH7RAWAWi"
      },
      "source": [
        "## Run the X_train_val_word2vec_features into our defined function for scoring \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_glove200d = pd.concat([\n",
        "#     metric_cv_stratified(mnb, X_train_val_glove_features, y_train_val, 5, 'MultinomialNB_glove50d'),\n",
        "     metric_cv_stratified(gnb, X_train_val_glove_features, y_train_val, 5, 'GaussianNB_glove200d'),\n",
        "     metric_cv_stratified(logit, X_train_val_glove_features, y_train_val, 5, 'LogisticRegression_glove200d'),\n",
        "     metric_cv_stratified(randomforest, X_train_val_glove_features, y_train_val, 5, 'RandomForest_glove200d'),\n",
        "     metric_cv_stratified(linearsvc, X_train_val_glove_features, y_train_val, 5, 'LinearSVC_glove200d')\n",
        "    ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcn2w_4rWAWp"
      },
      "source": [
        "with open('results_cv_straitified_glove200d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_cv_straitified_glove200d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3JgO0Ce3Gwz"
      },
      "source": [
        "files.download('results_cv_straitified_glove200d.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5IZP2G2WAWu"
      },
      "source": [
        "# Opening Results\n",
        "with open('results_cv_straitified_glove200d.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_glove200d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Iwwm2KWAW0"
      },
      "source": [
        "results_cv_straitified_glove200d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJIY9MwPWAW8"
      },
      "source": [
        "### Section 3.3.6: Using GloVe300d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPc0Ft-DWAW8"
      },
      "source": [
        "del glove_model_200d, results_cv_straitified_glove200d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XNRNnXRWAXA"
      },
      "source": [
        "## Using pre-trained GloVe\n",
        "# download from https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "num_features = 300 # depends on the pre-trained model you are loading\n",
        "\n",
        "glove_file = glove_dir = 'glove.6B.' + str(num_features) + 'd.txt'\n",
        "w2v_output_file = 'glove.6B.' + str(num_features) + 'd.txt.w2v'\n",
        "\n",
        "# The following utility converts file formats\n",
        "gensim.scripts.glove2word2vec.glove2word2vec(glove_file, w2v_output_file)\n",
        "\n",
        "# Now we can load it!\n",
        "glove_model_300d = gensim.models.KeyedVectors.load_word2vec_format(w2v_output_file, binary=False)\n",
        "\n",
        "# Pickle glove model so we don't have to do the above steps again and keep the damn glove.6b.50d in our folder\n",
        "with open('glove_model_' + str(num_features) + 'd.pickle', 'wb') as to_write:\n",
        "    pickle.dump(glove_model_300d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02246wQ3M-U"
      },
      "source": [
        "files.download('glove_model_' + str(num_features) + 'd.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Isq9xOMWAXE"
      },
      "source": [
        "# Load pickled glove_model\n",
        "with open('glove_model_300d.pickle', 'rb') as to_read:\n",
        "    glove_model_300d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxhvh4_WAXJ"
      },
      "source": [
        "# For Train_Val Complaints (80%)\n",
        "clean_train_val_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_train_val_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_train_val_glove_features = get_avg_feature_vecs(clean_train_val_complaints, glove_model_300d, num_features)\n",
        "\n",
        "# Already run above\n",
        "# # For Test Complaints (20%)\n",
        "# clean_test_complaints = []\n",
        "# for complaint in X_train_val:\n",
        "#     clean_test_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "# X_test_glove_features = get_avg_feature_vecs(clean_test_complaints, glove_model_200d, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRrT0B-0WAXN"
      },
      "source": [
        "## Run the X_train_val_word2vec_features into our defined function for scoring \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_glove300d= pd.concat([\n",
        "#     metric_cv_stratified(mnb, X_train_val_glove_features, y_train_val, 5, 'MultinomialNB_glove50d'),\n",
        "     metric_cv_stratified(gnb, X_train_val_glove_features, y_train_val, 5, 'GaussianNB_glove300d'),\n",
        "     metric_cv_stratified(logit, X_train_val_glove_features, y_train_val, 5, 'LogisticRegression_glove300d'),\n",
        "     metric_cv_stratified(randomforest, X_train_val_glove_features, y_train_val, 5, 'RandomForest_glove300d'),\n",
        "     metric_cv_stratified(linearsvc, X_train_val_glove_features, y_train_val, 5, 'LinearSVC_glove300d')\n",
        "    ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3cidGmWAXR"
      },
      "source": [
        "with open('results_cv_straitified_glove300d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_cv_straitified_glove300d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFU7sMeU3TV1"
      },
      "source": [
        "files.download('results_cv_straitified_glove300d.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IrF4C9WAXU"
      },
      "source": [
        "# Opening Results\n",
        "with open('results_cv_straitified_glove300d.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_glove300d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtPZyv_9WAXi"
      },
      "source": [
        "results_cv_straitified_glove300d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svwKQF73WAXm"
      },
      "source": [
        "### Section 3.3.7: Using GoogleNews Word2Vec300d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vU8htP0WAXo"
      },
      "source": [
        "del glove_model_300d, results_cv_straitified_glove300d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWIHIdDtWAXr"
      },
      "source": [
        "## Using pre-trained GoogleNews Word2Vec\n",
        "# download from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
        "\n",
        "num_features = 300 # depends on the pre-trained model you are loading\n",
        "\n",
        "# Path to where the word2vec file lives\n",
        "google_vec_file = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "# Load it!  This might take a few minutes...\n",
        "word2vec_model_300d = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)\n",
        "# it is just loading all the different weights (embedding) into python\n",
        "\n",
        "\n",
        "# Pickle word2vec 300d model so we don't have to do the above steps again and keep the damn file in our folder\n",
        "with open('word2vec_model_' + str(num_features) + 'd.pickle', 'wb') as to_write:\n",
        "    pickle.dump(word2vec_model_300d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KykB0-Ut3cet"
      },
      "source": [
        "files.download('word2vec_model_' + str(num_features) + 'd.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJztt63YWAXw"
      },
      "source": [
        "# Load pickled glove_model\n",
        "with open('word2vec_model_300d.pickle', 'rb') as to_read:\n",
        "    word2vec_model_300d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGS29qt5WAXz"
      },
      "source": [
        "# For Train_Val Complaints (80%)\n",
        "clean_train_val_complaints = []\n",
        "for complaint in X_train_val:\n",
        "    clean_train_val_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "X_train_val_glove_features = get_avg_feature_vecs(clean_train_val_complaints, word2vec_model_300d, num_features)\n",
        "\n",
        "# Already run above\n",
        "# # For Test Complaints (20%)\n",
        "# clean_test_complaints = []\n",
        "# for complaint in X_train_val:\n",
        "#     clean_test_complaints.append(complaint_to_wordlist(complaint, True))\n",
        "\n",
        "# X_test_glove_features = get_avg_feature_vecs(clean_test_complaints, glove_model_200d, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5VBpV6-WAX2"
      },
      "source": [
        "## Run the X_train_val_word2vec_features into our defined function for scoring \n",
        "\n",
        "# Initialize Model Object\n",
        "gnb = GaussianNB()\n",
        "mnb = MultinomialNB()\n",
        "logit = LogisticRegression(random_state=seed)\n",
        "randomforest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
        "linearsvc = LinearSVC()\n",
        "\n",
        "## We do NOT want these two. They take FOREVER to train AND predict\n",
        "# knn = KNeighborsClassifier()  \n",
        "# decisiontree = DecisionTreeClassifier(random_state=seed)\n",
        "\n",
        "# to concat all models\n",
        "results_cv_straitified_word2vec300d= pd.concat([\n",
        "#     metric_cv_stratified(mnb, X_train_val_glove_features, y_train_val, 5, 'MultinomialNB_glove50d'),\n",
        "     metric_cv_stratified(gnb, X_train_val_glove_features, y_train_val, 5, 'GaussianNB_word2vec300d'),\n",
        "     metric_cv_stratified(logit, X_train_val_glove_features, y_train_val, 5, 'LogisticRegression_word2vec300d'),\n",
        "     metric_cv_stratified(randomforest, X_train_val_glove_features, y_train_val, 5, 'RandomForest_word2vec300d'),\n",
        "     metric_cv_stratified(linearsvc, X_train_val_glove_features, y_train_val, 5, 'LinearSVC_word2vec300d')\n",
        "    ], axis=0).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mQkgamAWAX7"
      },
      "source": [
        "with open('results_cv_straitified_word2vec300d.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_cv_straitified_word2vec300d, to_write)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og__XgXS3ivN"
      },
      "source": [
        "files.download('results_cv_straitified_word2vec300d.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrpicKL-WAX-"
      },
      "source": [
        "# Opening Results\n",
        "with open('results_cv_straitified_word2vec300d.pickle', 'rb') as to_read:\n",
        "    results_cv_straitified_word2vec300d = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHMFtKvdWAYD"
      },
      "source": [
        "results_cv_straitified_word2vec300d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWSK9zj7WAYG"
      },
      "source": [
        "del word2vec_model_300d, results_cv_straitified_word2vec300d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F0halrnWAYK"
      },
      "source": [
        "### Section 3.3.8: Compiling Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21AVHQPhWAYL"
      },
      "source": [
        "results_compiled.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XfRrLDEWAYQ"
      },
      "source": [
        "results_compiled = pd.concat([results_cv_straitified_1gram,\n",
        "                           results_cv_straitified_glove50d,\n",
        "                           results_cv_straitified_glove100d,\n",
        "                           results_cv_straitified_glove200d,\n",
        "                           results_cv_straitified_glove300d,\n",
        "                           results_cv_straitified_word2vec300d]).reset_index().drop(['level_0','index'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGnCRW0PWAYT"
      },
      "source": [
        "with open('results_compiled.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_compiled, to_write)\n",
        "\n",
        "results_compiled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSy5kE06WAYY"
      },
      "source": [
        "#### Sorting in Descending order by Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnfGF59EWAYZ"
      },
      "source": [
        "# Sorting results to see which one gives us the best results\n",
        "results_compiled_sorted_by_accuracy = results_compiled.sort_values(by='Accuracy', ascending=False)\n",
        "\n",
        "with open('results_compiled_sorted_by_accuracy.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_compiled_sorted_by_accuracy, to_write)\n",
        "\n",
        "results_compiled_sorted_by_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRoPTFA3sWD"
      },
      "source": [
        "files.download('results_compiled_sorted_by_accuracy.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbfheF1cWAYw"
      },
      "source": [
        "# Retrieving the Model that provides the highest Accuracy\n",
        "results_highest_accuracy = results_compiled[results_compiled.Accuracy == results_compiled.Accuracy.max()]\n",
        "\n",
        "with open('results_highest_accuracy.pickle', 'wb') as to_write:\n",
        "    pickle.dump(results_highest_accuracy, to_write)\n",
        "\n",
        "results_highest_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhzgxl7CWAY1"
      },
      "source": [
        "# We see from the above that LogisiticRegression and LinearSVC on 1-gram gives pretty good results.\n",
        "# But LogReg is the best. So let's go with that!\n",
        "\n",
        "# Now let us do the final test!!\n",
        "# Pretty decent accuracy, actually.\n",
        "# I want to try 2-gram but my computer just simply won't be able to handle it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUpjkcBBWAZD"
      },
      "source": [
        "# Section 4: Final Test - 80% Train/Test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSrpI0AWAZE"
      },
      "source": [
        "## Section 4.1: Splitting to 80% Train and 20% unseen data\n",
        "#### Remember the 20% data we heldout earlier that we didn’t use in the cross-validation? Here comes the part where we use it as an estimate as to how well our model will perform on unseen data (i.e. how it would perform in real-life scenarios)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIzpxrzdWAZK"
      },
      "source": [
        "# Split the data into X and y data sets\n",
        "X, y = df.CONSUMER_COMPLAINT, df.PRODUCT_ID\n",
        "print('X shape:', X.shape, 'y shape:', y.shape)\n",
        "\n",
        "# For text classification, ALWAYS split data first before vectorizing.\n",
        "# This is because you don't want to cheat by having features (words) from the test data already being inside your train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
        "                                                            test_size=0.2,   # 80% train/cv, 20% test\n",
        "                                                            stratify=y,\n",
        "                                                            random_state=seed)\n",
        "print('X_train', X_train_val.shape)\n",
        "print('y_train', y_train_val.shape)\n",
        "print('X_test', X_test.shape)\n",
        "print('y_test', y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrNhjx3CWAZQ"
      },
      "source": [
        "# tfidf1 = 1-gram only. \n",
        "tfidf1 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                        min_df=5,\n",
        "                        stop_words='english')\n",
        "X_train_val_tfidf1 = tfidf1.fit_transform(X_train_val).toarray()\n",
        "X_test_tfidf1 = tfidf1.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qio7Dk-0G73r"
      },
      "source": [
        "# Loading our saved models\n",
        "# Consider training on the whole data instead now?\n",
        "# Pickle trained Model for use in Flask App\n",
        "with open('fitted_tfidf_to_use.pickle', 'wb') as to_write:\n",
        "    pickle.dump(tfidf1, to_write)\n",
        "with open('fitted_tfidf_to_use.pickle', 'rb') as to_read:  # loading the fitted tfidf with our 80% trained data\n",
        "    fitted_tfidf_to_use = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rwhXSgxWAZU"
      },
      "source": [
        "# Initializing our chosen logreg model\n",
        "logit = LogisticRegression()\n",
        "\n",
        "# Fitting our model\n",
        "logit_finalized = logit.fit(X_train_val_tfidf1, y_train_val)\n",
        "\n",
        "# Obtaining prediction\n",
        "y_pred = logit_finalized.predict(X_test_tfidf1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q--EsGd9WAZZ"
      },
      "source": [
        "# Pickle trained Model for use in Flask App\n",
        "with open('logit_finalized.pickle', 'wb') as to_write:\n",
        "    pickle.dump(logit_finalized, to_write)\n",
        "with open('logit_finalized.pickle', 'rb') as to_read:  # loading our model\n",
        "    logit_finalized = pickle.load(to_read)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1iYfXO0WAZi"
      },
      "source": [
        "# Create a function to calculate the error metrics, since we'll be doing this several times\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "def conf_matrix(actual, predicted):\n",
        "    \n",
        "    # Creates a dataframe of the ISSUE to their respective ISSUE_ID\n",
        "    category_id_df = df[['PRODUCT', 'PRODUCT_ID']].drop_duplicates()\n",
        "\n",
        "    # Dictionaries for future use. Creating our cheatsheets for what each encoded label represents.\n",
        "    category_to_id = dict(category_id_df.values) # Creates a ISSUE: ISSUE_ID key-value pair\n",
        "    id_to_category = dict(category_id_df[['PRODUCT_ID', 'PRODUCT']].values)  # Creates a ISSUE_ID: ISSUE key-value pair\n",
        "\n",
        "    conf_mat = confusion_matrix(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    g= sns.heatmap(conf_mat, \n",
        "                   annot=True, annot_kws={\"size\":10},\n",
        "                   cmap=plt.cm.Reds, square=True,\n",
        "                   fmt='d',\n",
        "                   xticklabels=category_id_df.ISSUE.values, \n",
        "                   yticklabels=category_id_df.ISSUE.values)\n",
        "    \n",
        "#     # Changing the size of the xticks and ytick labels\n",
        "#     ax.set_yticklabels(g.get_yticklabels(), rotation=90, size=10);\n",
        "#     ax.set_xticklabels(g.get_xticklabels(), size=10);\n",
        "    \n",
        "    # Changing axis orientation & setting titles\n",
        "    ax.set_xlabel('Prediction', size=15)\n",
        "    ax.set_ylabel('Actual', rotation=0, labelpad=40,size=15)\n",
        "\n",
        "#     plt.title(\"CONFUSION MATRIX - {}\\n\".format(name), size=16);\n",
        "    \n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + 0.5, top - 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKgl0aAHWAZs"
      },
      "source": [
        "'''with open('results_highest_accuracy.pickle', 'rb') as to_read:\n",
        "    results_highest_accuracy = pickle.load(to_read)\n",
        "\n",
        "results_highest_accuracy'''\n",
        "\n",
        "# Score model\n",
        "print(\"---------------------------------------------------------\")\n",
        "print(\"LogisticRegression (1-gram) 80% TRAIN/20% TEST SCORES:\")\n",
        "print(\"---------------------------------------------------------\")\n",
        "print('\\n')\n",
        "#print('Train/Cross-Validation Test Accuracy Score (also micro F1) for LogisticRegression: {:.4f}'.format(results_highest_accuracy.Accuracy.max()))\n",
        "print('Final Test Accuracy Score (also micro F1) for LogisticRegression: {:.4f}'.format(accuracy_score(y_test, y_pred)))\n",
        "print('\\n')\n",
        "print('Macro Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
        "print('Macro Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
        "print('Macro F1 score = {:.4f}'.format(f1_score(y_test, y_pred, average='macro')))\n",
        "print('\\n')\n",
        "print('Micro Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
        "print('Micro Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
        "print('Micro F1 score = {:.4f}'.format(f1_score(y_test, y_pred, average='micro')))\n",
        "print('\\n')\n",
        "print('Weighted Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
        "print('Weighted Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
        "print('Weighted F1 score = {:.4f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
        "print('\\n')\n",
        "print('Classification report for LogisticRegression (1-gram):\\n {}'.format(classification_report(y_test, \n",
        "                                                                                             y_pred,\n",
        "                                                                                             target_names=df_dc.PRODUCT.unique())))\n",
        "print('Confusion Matrix for LogisticRegression (1-gram):\\n'.format(conf_matrix(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6-8g83jWAZx"
      },
      "source": [
        "conf_matrix(y_test, y_pred)\n",
        "plt.savefig('confusion_matrix', transparent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EouXjoefWAaF"
      },
      "source": [
        "## Section 5.1: Random Complaint 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF3L9VHXWAaG",
        "scrolled": true
      },
      "source": [
        "# Using our models\n",
        "\n",
        "complaint = \"\"\"\n",
        "omg where is my money\n",
        "aosidnoiandnaiosdnasd This is bullshit!! where is my money!!!\n",
        "I TRANSFERRED IT BUT IT JUST DISAPPEARED INTO THIN AIR.\n",
        "WHAT KIND OF A BANK ARE YOU?\n",
        "ARE YOU A BANK OR A THIEF?\"\"\"\n",
        "\n",
        "# After fitting the tfidf vectorizor, then you can do transforms!\n",
        "new_vectorized_complaint = fitted_tfidf_to_use.transform([complaint])\n",
        "\n",
        "# Fitting vectorized complaint into model\n",
        "y_customized_prediction = logit_finalized.predict(new_vectorized_complaint)\n",
        "y_customized_prediction[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnrPeyFxWAaK"
      },
      "source": [
        "## Section 5.1: Random Complaint 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2hfCsT6WAaL"
      },
      "source": [
        "# Using our models\n",
        "\n",
        "complaint_2 = \"\"\"\n",
        "I borrowed money from you guys but it seems that the interest terms are unclear?\n",
        "Please shed some more light on this because i seem to be overcharged this month and I don't want to pay more than I should\n",
        "\"\"\"\n",
        "\n",
        "# After fitting the tfidf vectorizor, then you can do transforms!\n",
        "new_vectorized_complaint_2 = fitted_tfidf_to_use.transform([complaint_2])\n",
        "\n",
        "# Fitting vectorized complaint into model\n",
        "y_customized_prediction = logit_finalized.predict(new_vectorized_complaint_2)\n",
        "y_customized_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owhbHSSOWAaP"
      },
      "source": [
        "# Not bad! Our model actually does do a pretty good job at predicting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwMY3BF9WAaT"
      },
      "source": [
        "## Random Complaint 3 with no relevance at all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1gbBrxHWAaT"
      },
      "source": [
        "What about _completely_ irrelevant topics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM5P2ivPWAaV"
      },
      "source": [
        "# Using our models\n",
        "\n",
        "complaint_3 = \"\"\"I love macdonalds\"\"\"\n",
        "\n",
        "# After fitting the tfidf vectorizor, then you can do transforms!\n",
        "new_vectorized_complaint_3 = fitted_tfidf_to_use.transform([complaint_3])\n",
        "\n",
        "# Fitting vectorized complaint into model\n",
        "y_customized_prediction = logit_finalized.predict(new_vectorized_complaint_3)\n",
        "y_customized_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bXjGNiHWAaY"
      },
      "source": [
        "## Random Complaint 4 with no relevance at all"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BifJVNvFWAaZ"
      },
      "source": [
        "# Using our models\n",
        "\n",
        "complaint_4 = \"\"\"Pair programming is not fun\"\"\"\n",
        "\n",
        "# After fitting the tfidf vectorizor, then you can do transforms!\n",
        "new_vectorized_complaint_4 = fitted_tfidf_to_use.transform([complaint_4])\n",
        "\n",
        "# Fitting vectorized complaint into model\n",
        "y_customized_prediction = logit_finalized.predict(new_vectorized_complaint_4)\n",
        "y_customized_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVGljhtUWAae"
      },
      "source": [
        "# Section 6: Highest Occuring Words in each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbUq0jpWAae"
      },
      "source": [
        "# tfidf2 = unigram and bigram\n",
        "tfidf2 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                        min_df=5, \n",
        "                        ngram_range=(1,2), # we consider unigrams and bigrams\n",
        "                        stop_words='english')\n",
        "\n",
        "# We transform each complaint into a vector\n",
        "features = tfidf2.fit_transform(df_dc.CONSUMER_COMPLAINT).toarray()\n",
        "# Labelling our data\n",
        "labels = df_dc.ISSUE_ID\n",
        "\n",
        "print(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2kjZdPPWAai"
      },
      "source": [
        "category_to_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqiuDePpWAam"
      },
      "source": [
        "# Finding the three most correlated terms with each of the product categories\n",
        "N = 3\n",
        "for Product, category_id in sorted(category_to_id.items()):\n",
        "    features_chi2 = chi2(features, labels == category_id)\n",
        "    indices = np.argsort(features_chi2[0])\n",
        "    feature_names = np.array(tfidf2.get_feature_names())[indices]\n",
        "    \n",
        "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "    print(\"\\n==> %s:\" %(Product))\n",
        "    print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
        "    print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW-E1twTnYfM"
      },
      "source": [
        "## Productwise model building using Multiclassification methods on 'Issue' category\n",
        "## To build productwise multiclassification model on issue category, selecting each product and build the model\n",
        "## 1st Issue category model for Product='Debt collection' "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiSZBeVYKj4z"
      },
      "source": [
        "def product_issue_cat_multiclf(df_product_and_complaint_reduced, product):\n",
        "  global  product_nm, X, y, series, mask, X_train_val, X_test, y_train_val, y_test, tfidf1, X_train_val_tfidf1, tfidf2, X_train_val_tfidf2, X_test_tfidf1, X_test_tfidf2\n",
        "  df = pd.DataFrame()\n",
        "  product_nm = product\n",
        "  df = df_product_and_complaint_reduced[['PRODUCT','ISSUE','COMPLAINT_ID','PRODUCT_ID','Modified_text']].rename(columns={'Modified_text':'CONSUMER_COMPLAINT'})\n",
        "  df = df[(df['PRODUCT']==product_nm)]\n",
        "  series = pd.value_counts(df.ISSUE)\n",
        "  mask = (series/series.sum() * 100).lt(1)\n",
        "  df['ISSUE'] = np.where(df['ISSUE'].isin(series[mask].index),'Other',df['ISSUE'])\n",
        "  print(df[['PRODUCT','ISSUE']].value_counts(normalize=True)*100)\n",
        "  # Subsetting dataframe into columns useful for our text multi-classification problem\n",
        "  df = df[['ISSUE','CONSUMER_COMPLAINT']].copy()\n",
        "  df.CONSUMER_COMPLAINT = df.CONSUMER_COMPLAINT.astype(str)\n",
        "  df['ISSUE_ID'] = df['ISSUE'].factorize()[0]\n",
        "  # Split the data into X and y data sets\n",
        "  X, y = df.CONSUMER_COMPLAINT, df.ISSUE_ID\n",
        "  # For text classification, ALWAYS split data first before vectorizing.\n",
        "  # This is because you don't want to cheat by having features (words) from the test data already being inside your train data\n",
        "  X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, \n",
        "                                                              test_size=0.2,   # 80% train/cv, 20% test\n",
        "                                                              stratify=y,\n",
        "                                                              random_state=seed)\n",
        "  # tfidf1 = 1-gram only. \n",
        "  tfidf1 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                          min_df=5,\n",
        "                          stop_words='english')\n",
        "\n",
        "  X_train_val_tfidf1 = tfidf1.fit_transform(X_train_val).toarray()\n",
        "  X_test_tfidf1 = tfidf1.transform(X_test)\n",
        "\n",
        "  # tfidf2 = unigram and bigram\n",
        "  '''tfidf2 = TfidfVectorizer(sublinear_tf=True, # set to true to scale the term frequency in logarithmic scale.\n",
        "                            min_df=5, \n",
        "                            ngram_range=(1,2), # we consider unigrams and bigrams\n",
        "                            stop_words='english')\n",
        "    X_train_val_tfidf2 = tfidf2.fit_transform(X_train_val).toarray()\n",
        "    X_test_tfidf2 = tfidf2.transform(X_test)'''\n",
        "  \n",
        "  # Initializing our chosen logreg model\n",
        "  logit = LogisticRegression()\n",
        "\n",
        "  # Fitting our model\n",
        "  logit_finalized_1 = logit.fit(X_train_val_tfidf1, y_train_val)\n",
        "  #logit_finalized_2 = logit.fit(X_train_val_tfidf2, y_train_val)\n",
        "  # Obtaining prediction\n",
        "  y_pred_1 = logit_finalized_1.predict(X_test_tfidf1)\n",
        "  #y_pred_2 = logit_finalized_2.predict(X_test_tfidf2)\n",
        "  #return X_test_tfidf1, X_test_tfidf2, logit_finalized_1, logit_finalized_2, y_pred_1, y_pred_2, y_test\n",
        "  return X_test_tfidf1, logit_finalized_1, y_pred_1, y_test, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZll5XpwSUc4"
      },
      "source": [
        "# Create a function to calculate the error metrics, since we'll be doing this several times\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "def conf_matrix(df_product_and_complaint_reduced, product, y_test, y_pred):\n",
        "    global  product_nm\n",
        "    df = pd.DataFrame()\n",
        "    product_nm = product\n",
        "    df = df_product_and_complaint_reduced[['PRODUCT','ISSUE','COMPLAINT_ID','PRODUCT_ID','Modified_text']].rename(columns={'Modified_text':'CONSUMER_COMPLAINT'})\n",
        "    df = df[(df['PRODUCT']==product_nm)]\n",
        "    series = pd.value_counts(df.ISSUE)\n",
        "    mask = (series/series.sum() * 100).lt(1)\n",
        "    df['ISSUE'] = np.where(df['ISSUE'].isin(series[mask].index),'Other',df['ISSUE'])\n",
        "    df['ISSUE_ID'] = df['ISSUE'].factorize()[0]\n",
        "    # Creates a dataframe of the ISSUE to their respective ISSUE_ID\n",
        "    category_id_df = df[['ISSUE', 'ISSUE_ID']].drop_duplicates()\n",
        "\n",
        "    # Dictionaries for future use. Creating our cheatsheets for what each encoded label represents.\n",
        "    category_to_id = dict(category_id_df.values) # Creates a ISSUE: ISSUE_ID key-value pair\n",
        "    id_to_category = dict(category_id_df[['ISSUE_ID', 'ISSUE']].values)  # Creates a ISSUE_ID: ISSUE key-value pair\n",
        "\n",
        "    conf_mat = confusion_matrix(y_test, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8,8))\n",
        "    g= sns.heatmap(conf_mat, \n",
        "                   annot=True, annot_kws={\"size\":10},\n",
        "                   cmap=plt.cm.Reds, square=True,\n",
        "                   fmt='d',\n",
        "                   xticklabels=category_id_df.ISSUE.values, \n",
        "                   yticklabels=category_id_df.ISSUE.values)\n",
        "    \n",
        "#     # Changing the size of the xticks and ytick labels\n",
        "#     ax.set_yticklabels(g.get_yticklabels(), rotation=90, size=10);\n",
        "#     ax.set_xticklabels(g.get_xticklabels(), size=10);\n",
        "    \n",
        "    # Changing axis orientation & setting titles\n",
        "    ax.set_xlabel('Prediction', size=15)\n",
        "    ax.set_ylabel('Actual', rotation=0, labelpad=40,size=15)\n",
        "\n",
        "#     plt.title(\"CONFUSION MATRIX - {}\\n\".format(name), size=16);\n",
        "    \n",
        "    bottom, top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + 0.5, top - 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHVHKo0kKm_6"
      },
      "source": [
        "'''Debt collection                dc\n",
        "Mortgage                        mrg\n",
        "Credit card                     cc\n",
        "Loan                            ln\n",
        "Credit reporting                cr\n",
        "Checking or savings account     sa\n",
        "Bank account or service         ba\n",
        "Money transfers                 mt\n",
        "Prepaid card                     pc\n",
        "Other financial service           of'''\n",
        "\n",
        "product = 'Debt collection'\n",
        "prd_int = 'dc'\n",
        "#X_test_tfidf1, X_test_tfidf2, logit_finalized_1, logit_finalized_2, y_pred_1, y_pred_2, y_test = product_issue_cat_multiclass(df_product_and_complaint_reduced,product)\n",
        "X_test_tfidf1, logit_finalized_1, y_pred_1, y_test, df = product_issue_cat_multiclf(df_product_and_complaint_reduced,product)\n",
        "\n",
        "# Pickle trained Model for use in Flask App\n",
        "mdl_nm = prd_int+'_logit_finalized_1.pickle'\n",
        "with open(mdl_nm, 'wb') as to_write:\n",
        "    pickle.dump(logit_finalized_1, to_write)\n",
        "with open(mdl_nm, 'rb') as to_read:  # loading our model\n",
        "    logit_finalized_1 = pickle.load(to_read)\n",
        "\n",
        "'''# Pickle trained Model for use in Flask App\n",
        "mdl_nm = prd_int+'_logit_finalized_2.pickle'\n",
        "with open(mdl_nm, 'wb') as to_write:\n",
        "    pickle.dump(logit_finalized_2, to_write)\n",
        "with open(mdl_nm, 'rb') as to_read:  # loading our model\n",
        "    logit_finalized_2 = pickle.load(to_read)'''\n",
        "\n",
        "# Loading our saved models\n",
        "# Consider training on the whole data instead now?\n",
        "# Pickle trained Model for use in Flask App\n",
        "tfidf_nm = prd_int+'_fitted_tfidf_to_use.pickle'\n",
        "with open(tfidf_nm, 'wb') as to_write:\n",
        "    pickle.dump(tfidf1, to_write)\n",
        "with open(tfidf_nm, 'rb') as to_read:  # loading the fitted tfidf with our 80% trained data\n",
        "    fitted_tfidf_to_use = pickle.load(to_read)    \n",
        "\n",
        "# Score model\n",
        "print(\"---------------------------------------------------------\")\n",
        "print(\"LogisticRegression (1-gram) 80% TRAIN/20% TEST SCORES:\")\n",
        "print(\"---------------------------------------------------------\")\n",
        "print('\\n')\n",
        "#print('Train/Cross-Validation Test Accuracy Score (also micro F1) for LogisticRegression: {:.4f}'.format(results_highest_accuracy.Accuracy.max()))\n",
        "print('Final Test Accuracy Score (also micro F1) for LogisticRegression: {:.4f}'.format(accuracy_score(y_test, y_pred_1)))\n",
        "print('\\n')\n",
        "print('Macro Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred_1, average='macro')))\n",
        "print('Macro Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred_1, average='macro')))\n",
        "print('Macro F1 score = {:.4f}'.format(f1_score(y_test, y_pred_1, average='macro')))\n",
        "print('\\n')\n",
        "print('Micro Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred_1, average='micro')))\n",
        "print('Micro Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred_1, average='micro')))\n",
        "print('Micro F1 score = {:.4f}'.format(f1_score(y_test, y_pred_1, average='micro')))\n",
        "print('\\n')\n",
        "print('Weighted Precision Score for LogisticRegression: {:.4f}'.format(precision_score(y_test, y_pred_1, average='weighted')))\n",
        "print('Weighted Recall Score for LogisticRegression: {:.4f}'.format(recall_score(y_test, y_pred_1, average='weighted')))\n",
        "print('Weighted F1 score = {:.4f}'.format(f1_score(y_test, y_pred_1, average='weighted')))\n",
        "print('\\n')\n",
        "print('Classification report for LogisticRegression (1-gram):\\n {}'.format(classification_report(y_test, \n",
        "                                                                                             y_pred_1,\n",
        "                                                                                             target_names=df.ISSUE.unique())))\n",
        "print('Confusion Matrix for LogisticRegression (1-gram):\\n'.format(conf_matrix(df_product_and_complaint_reduced, product, y_test, y_pred_1)))\n",
        "plt.savefig('confusion_matrix', transparent=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}